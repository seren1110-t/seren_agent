import streamlit as st
from transformers import pipeline
import torch

st.title("ğŸ¤– ì‘ì€ LLM ëª¨ë¸ ë¡œë“œ í™•ì¸")

@st.cache_resource
def load_model():
    """ì‘ì€ LLM ëª¨ë¸ ë¡œë“œ"""
    try:
        # DistilGPT-2 (ì‘ì€ ëª¨ë¸, ì•½ 82MB)
        model = pipeline(
            "text-generation", 
            model="distilgpt2",
            device=0 if torch.cuda.is_available() else -1
        )
        return model, "âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!"
    except Exception as e:
        return None, f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"

# ëª¨ë¸ ë¡œë“œ
with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘..."):
    model, status = load_model()

st.write(status)

if model:
    # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
    st.subheader("í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸")
    
    prompt = st.text_input("í”„ë¡¬í”„íŠ¸ ì…ë ¥:", "Hello, I am")
    
    if st.button("ìƒì„±"):
        if prompt:
            with st.spinner("ìƒì„± ì¤‘..."):
                result = model(prompt, max_length=50, num_return_sequences=1)
                st.write("**ìƒì„± ê²°ê³¼:**")
                st.write(result[0]['generated_text'])
        else:
            st.warning("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# ì‹œìŠ¤í…œ ì •ë³´
st.sidebar.write("**ì‹œìŠ¤í…œ ì •ë³´:**")
st.sidebar.write(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    st.sidebar.write(f"GPU: {torch.cuda.get_device_name(0)}")
st.sidebar.write(f"PyTorch ë²„ì „: {torch.__version__}")
