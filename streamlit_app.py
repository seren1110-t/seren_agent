# kospi_research_app.py
import os
import json

# PyTorchì™€ Streamlit í˜¸í™˜ì„± ë¬¸ì œ í•´ê²° (ë°˜ë“œì‹œ ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

# PyTorch ì„í¬íŠ¸ ë° í˜¸í™˜ì„± ìˆ˜ì •
import torch
try:
    torch.classes.__path__ = [os.path.join(torch.__path__[0], 'classes')]
except Exception:
    try:
        if hasattr(torch, 'classes'):
            torch.classes.__path__._path = [os.path.join(torch.__path__[0], 'classes')]
    except Exception:
        pass

import streamlit as st
import pandas as pd
import sqlite3
import gdown
import zipfile
import tarfile
import torch.quantization
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig
import numpy as np
import gc

st.set_page_config(page_title="ğŸ“ˆ KOSPI Analyst AI", layout="wide")

def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def find_best_checkpoint(base_path, preferred_checkpoint="checkpoint-200"):
    """ìµœì ì˜ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì°¾ê¸°"""
    # ì„ í˜¸í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ë¨¼ì € í™•ì¸
    preferred_path = os.path.join(base_path, preferred_checkpoint)
    if os.path.exists(preferred_path):
        adapter_config = os.path.join(preferred_path, "adapter_config.json")
        adapter_model = os.path.join(preferred_path, "adapter_model.safetensors")
        if not os.path.exists(adapter_model):
            adapter_model = os.path.join(preferred_path, "adapter_model.bin")
        
        if os.path.exists(adapter_config) and os.path.exists(adapter_model):
            return preferred_path, preferred_checkpoint
    
    # ì„ í˜¸í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì²´í¬í¬ì¸íŠ¸ íƒìƒ‰
    checkpoint_dirs = []
    if os.path.exists(base_path):
        for item in os.listdir(base_path):
            if item.startswith("checkpoint-") and os.path.isdir(os.path.join(base_path, item)):
                checkpoint_dirs.append(item)
    
    # ì²´í¬í¬ì¸íŠ¸ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ë²ˆí˜¸ë¶€í„°)
    checkpoint_dirs.sort(key=lambda x: int(x.split("-")[1]), reverse=True)
    
    for checkpoint_dir in checkpoint_dirs:
        checkpoint_path = os.path.join(base_path, checkpoint_dir)
        adapter_config = os.path.join(checkpoint_path, "adapter_config.json")
        adapter_model = os.path.join(checkpoint_path, "adapter_model.safetensors")
        if not os.path.exists(adapter_model):
            adapter_model = os.path.join(checkpoint_path, "adapter_model.bin")
        
        if os.path.exists(adapter_config) and os.path.exists(adapter_model):
            return checkpoint_path, checkpoint_dir
    
    return None, None

@st.cache_resource
def download_and_load_models():
    """Google Driveì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° CPU ìµœì í™” QLoRA ë¡œë“œ"""
    
    # Google Drive íŒŒì¼ ID
    base_model_id = "1CGpO7EO64hkUTU_eQQuZXbh-R84inkIc"
    qlora_adapter_id = "1l2F6a5HpmEmdOwTKOpu5UNRQG_jrXeW0"
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        cleanup_memory()
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        status_text.text("ğŸ”„ ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (1/8)")
        progress_bar.progress(10)
        
        if not os.path.exists("./base_model"):
            base_model_url = f"https://drive.google.com/uc?id={base_model_id}"
            gdown.download(base_model_url, "./my_base_model.tar.gz", quiet=False)
            
            with tarfile.open("./my_base_model.tar.gz", "r:gz") as tar:
                tar.extractall("./base_model/")
        
        # QLoRA ì–´ëŒ‘í„° ë‹¤ìš´ë¡œë“œ
        status_text.text("ğŸ”„ QLoRA ì–´ëŒ‘í„° ë‹¤ìš´ë¡œë“œ ì¤‘... (2/8)")
        progress_bar.progress(20)
        
        if not os.path.exists("./qlora_adapter"):
            qlora_url = f"https://drive.google.com/uc?id={qlora_adapter_id}"
            gdown.download(qlora_url, "./qlora_results.zip", quiet=False)
            
            with zipfile.ZipFile("./qlora_results.zip", 'r') as zip_ref:
                zip_ref.extractall("./qlora_adapter/")
        
        # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ í™•ì¸
        status_text.text("ğŸ”§ QLoRA ì²´í¬í¬ì¸íŠ¸ í™•ì¸ ì¤‘... (3/8)")
        progress_bar.progress(30)
        
        # checkpoint-200ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¾ê¸°
        adapter_path, checkpoint_name = find_best_checkpoint("./qlora_adapter", "checkpoint-200")
        
        if adapter_path is None:
            st.error("âŒ QLoRA ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
        
        st.info(f"âœ… ì‚¬ìš©í•  ì²´í¬í¬ì¸íŠ¸: {checkpoint_name}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ í‘œì‹œ
        try:
            with open(os.path.join(adapter_path, "adapter_config.json"), 'r') as f:
                adapter_config = json.load(f)
            st.info(f"ğŸ“‹ LoRA ì„¤ì •: r={adapter_config.get('r', 'N/A')}, alpha={adapter_config.get('lora_alpha', 'N/A')}")
        except:
            pass
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ (ë² ì´ìŠ¤ ëª¨ë¸ì—ì„œ)
        status_text.text("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘... (4/8)")
        progress_bar.progress(40)
        
        tokenizer = AutoTokenizer.from_pretrained(
            "./base_model",
            trust_remote_code=True,
            use_fast=False,  # CPU í™˜ê²½ì—ì„œ ì•ˆì •ì„± ìš°ì„ 
            padding_side="right"
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (CPU ìµœì í™”)
        status_text.text("ğŸ§  ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘ (CPU ìµœì í™”)... (5/8)")
        progress_bar.progress(50)
        
        base_model = AutoModelForCausalLM.from_pretrained(
            "./base_model",
            torch_dtype=torch.float32,  # CPUì—ì„œëŠ” float32 ì‚¬ìš©
            device_map="cpu",
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            use_cache=False,  # ë©”ëª¨ë¦¬ ì ˆì•½
            torch_compile=False  # CPUì—ì„œëŠ” ì»´íŒŒì¼ ë¹„í™œì„±í™”
        )
        
        cleanup_memory()
        
        # QLoRA ì–´ëŒ‘í„° ì„¤ì • í™•ì¸
        status_text.text("ğŸ”§ QLoRA ì–´ëŒ‘í„° ì„¤ì • í™•ì¸ ì¤‘... (6/8)")
        progress_bar.progress(60)
        
        try:
            peft_config = PeftConfig.from_pretrained(adapter_path)
            st.info(f"âœ… PEFT ì„¤ì •: {peft_config.task_type}, target_modules={len(peft_config.target_modules)}ê°œ")
        except Exception as e:
            st.warning(f"PeftConfig ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # QLoRA ì–´ëŒ‘í„° ì ìš©
        status_text.text(f"ğŸ”§ {checkpoint_name} ì–´ëŒ‘í„° ì ìš© ì¤‘... (7/8)")
        progress_bar.progress(70)
        
        model = PeftModel.from_pretrained(
            base_model, 
            adapter_path,
            torch_dtype=torch.float32,
            is_trainable=False  # ì¶”ë¡  ì „ìš©
        )
        
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        model.eval()
        
        cleanup_memory()
        
        # CPU ë™ì  ì–‘ìí™” ì ìš©
        status_text.text("âš¡ CPU ë™ì  ì–‘ìí™” ì ìš© ì¤‘... (8/8)")
        progress_bar.progress(80)
        
        try:
            with torch.no_grad():
                # Linear ë ˆì´ì–´ë§Œ INT8ë¡œ ì–‘ìí™”
                quantized_model = torch.quantization.quantize_dynamic(
                    model,
                    {torch.nn.Linear},
                    dtype=torch.qint8,
                    inplace=False
                )
            model = quantized_model
            st.success("âœ… CPU ë™ì  ì–‘ìí™” ì ìš© ì™„ë£Œ!")
        except Exception as e:
            st.warning(f"ë™ì  ì–‘ìí™” ì‹¤íŒ¨, ì›ë³¸ ëª¨ë¸ ì‚¬ìš©: {e}")
        
        progress_bar.progress(90)
        
        # ëª¨ë¸ ì •ë³´ í‘œì‹œ
        def get_model_size_safe(model):
            try:
                param_size = sum(p.numel() * p.element_size() for p in model.parameters())
                buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
                return (param_size + buffer_size) / 1024 / 1024
            except:
                return 0
        
        model_size = get_model_size_safe(model)
        
        # ì–´ëŒ‘í„° ì •ë³´ í‘œì‹œ
        if hasattr(model, 'peft_config') and model.peft_config:
            config = list(model.peft_config.values())[0]
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ëª¨ë¸ í¬ê¸°", f"{model_size:.1f} MB")
            with col2:
                st.metric("ì²´í¬í¬ì¸íŠ¸", checkpoint_name.split("-")[1])
            with col3:
                st.metric("LoRA Rank", f"{config.r}")
            with col4:
                st.metric("LoRA Alpha", f"{config.lora_alpha}")
        
        progress_bar.progress(100)
        status_text.text("âœ… QLoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        st.success(f"âš¡ CPU ìµœì í™”ëœ {checkpoint_name} QLoRA ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            for temp_file in ["./my_base_model.tar.gz", "./qlora_results.zip"]:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
        except:
            pass
        
        cleanup_memory()
        
        # UI ì •ë¦¬
        progress_bar.empty()
        status_text.empty()
        
        return model, tokenizer
        
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        progress_bar.empty()
        status_text.empty()
        cleanup_memory()
        return None, None

@st.cache_data
def load_data(db_name="financial_data.db", table_name="financial_data"):
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    try:
        conn = sqlite3.connect(db_name)
        df = pd.read_sql(f"SELECT * FROM {table_name}", conn)
        conn.close()
        
        # ìˆ«ì ì»¬ëŸ¼ë“¤ì„ numericìœ¼ë¡œ ë³€í™˜í•˜ê³  NaN ì²˜ë¦¬
        numeric_columns = ['PER_ìµœê·¼', 'PBR_ìµœê·¼', 'ROE_ìµœê·¼', 'ë¶€ì±„ë¹„ìœ¨_ìµœê·¼', 'í˜„ì¬ê°€', 
                          'ìœ ë³´ìœ¨_ìµœê·¼', 'ë§¤ì¶œì•¡_ìµœê·¼', 'ì˜ì—…ì´ìµ_ìµœê·¼', 'ìˆœì´ìµ_ìµœê·¼']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col] = df[col].fillna(0)
        
        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def generate_ai_response(model, tokenizer, question, company_data):
    """CPU ìµœì í™”ëœ QLoRA ëª¨ë¸ì„ ì‚¬ìš©í•œ AI ì‘ë‹µ ìƒì„±"""
    if model is None or tokenizer is None:
        return "AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    # QLoRA íŒŒì¸íŠœë‹ í˜•ì‹ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„± (ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸ ì½”ë“œ ì°¸ê³ )
    company_info = f"""ì¢…ëª©ëª…: {company_data['ì¢…ëª©ëª…']}
í‹°ì»¤: {company_data['í‹°ì»¤']}
í˜„ì¬ê°€: {company_data['í˜„ì¬ê°€']:,.0f}ì›
PER: {company_data['PER_ìµœê·¼']:.2f}
PBR: {company_data['PBR_ìµœê·¼']:.2f}
ROE: {company_data['ROE_ìµœê·¼']:.2f}%
ë¶€ì±„ë¹„ìœ¨: {company_data['ë¶€ì±„ë¹„ìœ¨_ìµœê·¼']:.2f}%"""
    
    # QLoRA íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì‚¬ìš©
    prompt = f"""ì§ˆë¬¸: {question}
ì •ë³´: {company_info}
ë‹µë³€:"""
    
    try:
        # CPU ìµœì í™”ëœ í† í°í™”
        inputs = tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512,  # CPUì—ì„œëŠ” ì§§ì€ ê¸¸ì´ ì‚¬ìš©
            padding=True,
            add_special_tokens=True
        )
        
        # CPUì—ì„œ ì•ˆì „í•œ ì¶”ë¡ 
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=150,  # CPUì—ì„œëŠ” í† í° ìˆ˜ ì œí•œ
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                num_beams=1,  # CPUì—ì„œëŠ” beam search ë¹„í™œì„±í™”
                early_stopping=True,
                use_cache=False,  # ë©”ëª¨ë¦¬ ì ˆì•½
                repetition_penalty=1.1,
                no_repeat_ngram_size=2  # ë°˜ë³µ ë°©ì§€
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = response[len(prompt):].strip()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del inputs, outputs
        cleanup_memory()
        
        return generated_text if generated_text else "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    except Exception as e:
        cleanup_memory()
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def get_initial(korean_char):
    """í•œê¸€ ì´ˆì„± ì¶”ì¶œ"""
    try:
        ch_code = ord(korean_char) - ord('ê°€')
        if 0 <= ch_code < 11172:
            cho = ch_code // 588
            return ['ã„±','ã„²','ã„´','ã„·','ã„¸','ã„¹','ã…','ã…‚','ã…ƒ','ã……','ã…†','ã…‡','ã…ˆ','ã…‰','ã…Š','ã…‹','ã…Œ','ã…','ã…'][cho]
    except:
        pass
    return ""

# AI ëª¨ë¸ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
if 'model_loaded' not in st.session_state:
    st.info("ğŸ¤– CPU ìµœì í™”ëœ QLoRA checkpoint-200 ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
    model, tokenizer = download_and_load_models()
    st.session_state.model = model
    st.session_state.tokenizer = tokenizer
    st.session_state.model_loaded = True
    
    if model is not None:
        st.success("âœ… CPU ìµœì í™”ëœ QLoRA checkpoint-200 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì´ì œ ì „ë¬¸ì ì¸ ê¸ˆìœµ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        st.rerun()

# ë°ì´í„° ë¡œë“œ
df = load_data()

if df.empty:
    st.error("âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# ì‚¬ì´ë“œë°” í•„í„°
st.sidebar.header("ğŸ“‚ í•„í„° ì˜µì…˜")

# ì´ˆì„± í•„í„°
initials = ['ì „ì²´', 'ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
selected_initial = st.sidebar.selectbox("ğŸ”¡ ì¢…ëª©ëª… ì´ˆì„±:", initials)

if selected_initial != "ì „ì²´":
    df = df[df["ì¢…ëª©ëª…"].apply(lambda x: get_initial(x[0]) == selected_initial if x else "")]

# í…ìŠ¤íŠ¸ ê²€ìƒ‰
search_term = st.sidebar.text_input("ğŸ” ì¢…ëª©ëª… ë˜ëŠ” í‹°ì»¤ ê²€ìƒ‰")
if search_term:
    mask1 = df["ì¢…ëª©ëª…"].str.contains(search_term, case=False, na=False)
    mask2 = df["í‹°ì»¤"].str.contains(search_term, case=False, na=False)
    df = df[mask1 | mask2]

ì¢…ëª©_list = df["ì¢…ëª©ëª…"].tolist()

if not ì¢…ëª©_list:
    st.warning("âŒ ì¡°ê±´ì— ë§ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

ì„ íƒí•œ_ì¢…ëª© = st.sidebar.selectbox("ğŸ“Œ ì¢…ëª© ì„ íƒ:", ì¢…ëª©_list)
ì¢…ëª©_df = df[df["ì¢…ëª©ëª…"] == ì„ íƒí•œ_ì¢…ëª©].iloc[0]

# ë©”ì¸ ì»¨í…ì¸ 
st.title(f"ğŸ“Š {ì„ íƒí•œ_ì¢…ëª©} ({ì¢…ëª©_df['í‹°ì»¤']}) QLoRA-200 AI ë¶„ì„")

col1, col2 = st.columns(2)

with col1:
    st.metric("í˜„ì¬ê°€", f"{ì¢…ëª©_df['í˜„ì¬ê°€']:,.0f}ì›")
    st.metric("ROE (ìµœê·¼)", f"{ì¢…ëª©_df['ROE_ìµœê·¼']:.2f}%")
    st.metric("PER (ìµœê·¼)", f"{ì¢…ëª©_df['PER_ìµœê·¼']:.2f}")
    st.metric("PBR (ìµœê·¼)", f"{ì¢…ëª©_df['PBR_ìµœê·¼']:.2f}")
    st.metric("ë¶€ì±„ë¹„ìœ¨", f"{ì¢…ëª©_df['ë¶€ì±„ë¹„ìœ¨_ìµœê·¼']:.2f}%")

with col2:
    # ì•ˆì „í•œ ë©”íŠ¸ë¦­ í‘œì‹œ
    metrics = [
        ("ìœ ë³´ìœ¨", "ìœ ë³´ìœ¨_ìµœê·¼", "%"),
        ("ë§¤ì¶œì•¡", "ë§¤ì¶œì•¡_ìµœê·¼", "ì›"),
        ("ì˜ì—…ì´ìµ", "ì˜ì—…ì´ìµ_ìµœê·¼", "ì›"),
        ("ìˆœì´ìµ", "ìˆœì´ìµ_ìµœê·¼", "ì›")
    ]
    
    for label, col_name, unit in metrics:
        try:
            if col_name in ì¢…ëª©_df and pd.notna(ì¢…ëª©_df[col_name]):
                if unit == "ì›":
                    st.metric(label, f"{ì¢…ëª©_df[col_name]:,.0f}{unit}")
                else:
                    st.metric(label, f"{ì¢…ëª©_df[col_name]:.2f}{unit}")
            else:
                st.metric(label, "ë°ì´í„° ì—†ìŒ")
        except:
            st.metric(label, "ë°ì´í„° ì—†ìŒ")

# ê·¸ë˜í”„
st.markdown("### ğŸ“ˆ ì£¼ê°€ ì¶”ì´")
price_cols = [col for col in df.columns if col.isdigit() and len(col) == 8]
if price_cols:
    try:
        price_series = ì¢…ëª©_df[price_cols].astype(float)
        price_series.index = pd.to_datetime(price_cols, format='%Y%m%d')
        chart_df = price_series.reset_index().rename(columns={'index': 'ë‚ ì§œ'})
        st.line_chart(chart_df.set_index("ë‚ ì§œ"))
    except:
        st.info("ì£¼ê°€ ì°¨íŠ¸ ë°ì´í„°ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ë‰´ìŠ¤
st.markdown("### ğŸ“° ìµœê·¼ ë‰´ìŠ¤")
if "ìµœì‹ ë‰´ìŠ¤" in ì¢…ëª©_df and isinstance(ì¢…ëª©_df["ìµœì‹ ë‰´ìŠ¤"], str) and ì¢…ëª©_df["ìµœì‹ ë‰´ìŠ¤"].strip():
    for i, link in enumerate(ì¢…ëª©_df["ìµœì‹ ë‰´ìŠ¤"].splitlines(), 1):
        if link.strip():
            st.markdown(f"{i}. [ë‰´ìŠ¤ ë§í¬]({link.strip()})")
else:
    st.info("ìµœê·¼ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

# AI ë¦¬ì„œì¹˜ ì§ˆì˜
st.markdown("### ğŸ¤– QLoRA Checkpoint-200 AI ë¦¬ì„œì¹˜ ì§ˆì˜")

# ë¯¸ë¦¬ ì •ì˜ëœ ì§ˆë¬¸ë“¤
preset_questions = [
    "ì´ ì¢…ëª©ì˜ íˆ¬ì ë§¤ë ¥ë„ëŠ” ì–´ë–¤ê°€ìš”?",
    "PERê³¼ PBR ì§€í‘œë¥¼ ì–´ë–»ê²Œ í•´ì„í•´ì•¼ í•˜ë‚˜ìš”?",
    "í˜„ì¬ ì¬ë¬´ ìƒíƒœì˜ ê°•ì ê³¼ ì•½ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ì´ ì¢…ëª©ì˜ ë¦¬ìŠ¤í¬ ìš”ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ë™ì¢… ì—…ê³„ ëŒ€ë¹„ ê²½ìŸë ¥ì€ ì–´ë–¤ê°€ìš”?"
]

col1, col2 = st.columns([3, 1])

with col1:
    user_question = st.text_input("ğŸ§  ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:", 
                                placeholder="ì˜ˆ: ì´ ì¢…ëª©ì˜ PERì´ ë†’ìœ¼ë©´ ì–´ë–¤ í•´ì„ì´ ê°€ëŠ¥í•´?")

with col2:
    selected_preset = st.selectbox("ğŸ“‹ ë¯¸ë¦¬ ì •ì˜ëœ ì§ˆë¬¸:", ["ì§ì ‘ ì…ë ¥"] + preset_questions)

if selected_preset != "ì§ì ‘ ì…ë ¥":
    user_question = selected_preset

if user_question:
    if st.session_state.get('model') is not None:
        if st.button("ğŸ” QLoRA-200 AI ë¶„ì„ ìš”ì²­", type="primary"):
            with st.spinner("ğŸ¤– QLoRA checkpoint-200 ëª¨ë¸ì´ CPUì—ì„œ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                ai_response = generate_ai_response(
                    st.session_state.model, 
                    st.session_state.tokenizer, 
                    user_question, 
                    ì¢…ëª©_df
                )
            
            st.markdown("#### ğŸ¯ QLoRA-200 AI ë¶„ì„ ê²°ê³¼")
            st.markdown(ai_response)
            
            # ë¶„ì„ ê²°ê³¼ ì €ì¥ ì˜µì…˜
            analysis_text = f"""
# {ì„ íƒí•œ_ì¢…ëª©} QLoRA Checkpoint-200 AI ë¶„ì„ ê²°ê³¼

**ì§ˆë¬¸:** {user_question}

**QLoRA-200 AI ë¶„ì„:**
{ai_response}

**ê¸°ë³¸ ì •ë³´:**
- ì¢…ëª©ëª…: {ì¢…ëª©_df['ì¢…ëª©ëª…']}
- í‹°ì»¤: {ì¢…ëª©_df['í‹°ì»¤']}
- í˜„ì¬ê°€: {ì¢…ëª©_df['í˜„ì¬ê°€']:,.0f}ì›
- PER: {ì¢…ëª©_df['PER_ìµœê·¼']:.2f}
- PBR: {ì¢…ëª©_df['PBR_ìµœê·¼']:.2f}
- ROE: {ì¢…ëª©_df['ROE_ìµœê·¼']:.2f}%

*ë³¸ ë¶„ì„ì€ QLoRA checkpoint-200ìœ¼ë¡œ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""
            
            st.download_button(
                label="ğŸ“¥ QLoRA-200 ë¶„ì„ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ",
                data=analysis_text,
                file_name=f"{ì„ íƒí•œ_ì¢…ëª©}_QLoRA200_AIë¶„ì„_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.md",
                mime="text/markdown"
            )
    else:
        st.warning("ğŸ” QLoRA-200 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# QLoRA ì²´í¬í¬ì¸íŠ¸ ì •ë³´ í‘œì‹œ
if st.session_state.get('model_loaded'):
    with st.expander("âš¡ QLoRA Checkpoint-200 ëª¨ë¸ ì •ë³´"):
        st.markdown("""
        **QLoRA Checkpoint-200 ëª¨ë¸ íŠ¹ì§•:**
        - ğŸ”¹ **ì²´í¬í¬ì¸íŠ¸**: checkpoint-200 (ìµœì í™”ëœ í›ˆë ¨ ë‹¨ê³„)
        - ğŸ”¹ **QLoRA íŒŒì¸íŠœë‹**: ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” í•™ìŠµ ì™„ë£Œ
        - ğŸ”¹ **CPU ë™ì  ì–‘ìí™”**: INT8 ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
        - ğŸ”¹ **LoRA ì–´ëŒ‘í„°**: íš¨ìœ¨ì ì¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë°©ì‹
        - ğŸ”¹ **ë©”ëª¨ë¦¬ ìµœì í™”**: CPU í™˜ê²½ì— íŠ¹í™”ëœ ì¶”ë¡  ìµœì í™”
        
        **ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­:**
        - ë² ì´ìŠ¤ ëª¨ë¸: Llama ê¸°ë°˜ ëª¨ë¸
        - ì–‘ìí™”: 4bit â†’ INT8 ë™ì  ì–‘ìí™”
        - ì–´ëŒ‘í„°: LoRA (Low-Rank Adaptation)
        - í›ˆë ¨ ë°ì´í„°: ê¸ˆìœµ Q&A ë°ì´í„°ì…‹
        - ìµœì í™”: CPU ì¶”ë¡ ì— íŠ¹í™”ëœ ì„¤ì •
        """)
