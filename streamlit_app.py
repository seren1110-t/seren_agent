# kospi_research_app.py
import streamlit as st
import pandas as pd
import sqlite3
import requests
from io import StringIO
import gdown
import os
import zipfile
import tarfile
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
import numpy as np

st.set_page_config(page_title="ğŸ“ˆ KOSPI Analyst AI", layout="wide")

# Google Drive íŒŒì¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
@st.cache_resource
def download_and_load_models():
    """Google Driveì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ"""
    
    # Google Drive íŒŒì¼ ID (ê³µìœ  ë§í¬ì—ì„œ ì¶”ì¶œ)
    base_model_id = "YOUR_BASE_MODEL_FILE_ID"  # my_base_model.tar.gzì˜ íŒŒì¼ ID
    qlora_adapter_id = "YOUR_QLORA_ADAPTER_FILE_ID"  # qlora_results.zipì˜ íŒŒì¼ ID
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        status_text.text("ğŸ”„ ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (1/4)")
        progress_bar.progress(25)
        
        if not os.path.exists("./base_model"):
            base_model_url = f"https://drive.google.com/uc?id={base_model_id}"
            gdown.download(base_model_url, "./my_base_model.tar.gz", quiet=False)
            
            # ì••ì¶• í•´ì œ
            with tarfile.open("./my_base_model.tar.gz", "r:gz") as tar:
                tar.extractall("./base_model/")
        
        # QLoRA ì–´ëŒ‘í„° ë‹¤ìš´ë¡œë“œ
        status_text.text("ğŸ”„ QLoRA ì–´ëŒ‘í„° ë‹¤ìš´ë¡œë“œ ì¤‘... (2/4)")
        progress_bar.progress(50)
        
        if not os.path.exists("./qlora_adapter"):
            qlora_url = f"https://drive.google.com/uc?id={qlora_adapter_id}"
            gdown.download(qlora_url, "./qlora_results.zip", quiet=False)
            
            # ì••ì¶• í•´ì œ
            with zipfile.ZipFile("./qlora_results.zip", 'r') as zip_ref:
                zip_ref.extractall("./qlora_adapter/")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        status_text.text("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘... (3/4)")
        progress_bar.progress(75)
        
        tokenizer = AutoTokenizer.from_pretrained("./base_model")
        
        # ëª¨ë¸ ë¡œë“œ ë° ì–´ëŒ‘í„° ì ìš©
        status_text.text("ğŸ§  AI ëª¨ë¸ ë¡œë“œ ì¤‘... (4/4)")
        progress_bar.progress(90)
        
        base_model = AutoModelForCausalLM.from_pretrained(
            "./base_model",
            torch_dtype=torch.float16,
            device_map="cpu",
            low_cpu_mem_usage=True
        )
        
        # QLoRA ì–´ëŒ‘í„° ì ìš©
        model = PeftModel.from_pretrained(base_model, "./qlora_adapter")
        
        progress_bar.progress(100)
        status_text.text("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        
        # UI ì •ë¦¬
        progress_bar.empty()
        status_text.empty()
        
        return model, tokenizer
        
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

@st.cache_data
def load_data(db_name="financial_data.db", table_name="financial_data"):
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    try:
        conn = sqlite3.connect(db_name)
        df = pd.read_sql(f"SELECT * FROM {table_name}", conn)
        conn.close()
        
        # ìˆ«ì ì»¬ëŸ¼ë“¤ì„ numericìœ¼ë¡œ ë³€í™˜í•˜ê³  NaN ì²˜ë¦¬
        numeric_columns = ['PER_ìµœê·¼', 'PBR_ìµœê·¼', 'ROE_ìµœê·¼', 'ë¶€ì±„ë¹„ìœ¨_ìµœê·¼', 'í˜„ì¬ê°€']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col] = df[col].fillna(0)  # NaNì„ 0ìœ¼ë¡œ ëŒ€ì²´
        
        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def generate_ai_response(model, tokenizer, question, company_data):
    """AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""
    if model is None or tokenizer is None:
        return "AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    # íšŒì‚¬ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    company_info = f"""
    ì¢…ëª©ëª…: {company_data['ì¢…ëª©ëª…']}
    í‹°ì»¤: {company_data['í‹°ì»¤']}
    í˜„ì¬ê°€: {company_data['í˜„ì¬ê°€']}
    PER: {company_data['PER_ìµœê·¼']}
    PBR: {company_data['PBR_ìµœê·¼']}
    ROE: {company_data['ROE_ìµœê·¼']}
    ë¶€ì±„ë¹„ìœ¨: {company_data['ë¶€ì±„ë¹„ìœ¨_ìµœê·¼']}
    """
    
    prompt = f"""ë‹¤ìŒì€ {company_data['ì¢…ëª©ëª…']}ì˜ ì¬ë¬´ ì •ë³´ì…ë‹ˆë‹¤:

{company_info}

ì§ˆë¬¸: {question}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ì¦ê¶Œ ë¶„ì„ê°€ ê´€ì ì—ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""
    
    try:
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°
        generated_text = response[len(prompt):].strip()
        
        return generated_text
        
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def get_initial(korean_char):
    """í•œê¸€ ì´ˆì„± ì¶”ì¶œ"""
    ch_code = ord(korean_char) - ord('ê°€')
    if 0 <= ch_code < 11172:
        cho = ch_code // 588
        return ['ã„±','ã„²','ã„´','ã„·','ã„¸','ã„¹','ã…','ã…‚','ã…ƒ','ã……','ã…†','ã…‡','ã…ˆ','ã…‰','ã…Š','ã…‹','ã…Œ','ã…','ã…'][cho]
    return ""

def safe_between_filter(series, min_val, max_val):
    """ì•ˆì „í•œ between í•„í„°ë§ (NaN ê°’ ì²˜ë¦¬)"""
    try:
        # NaNì´ ì•„ë‹Œ ê°’ë“¤ë§Œ í•„í„°ë§
        mask = series.notna() & (series >= min_val) & (series <= max_val)
        return mask
    except:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëª¨ë“  ê°’ì„ Trueë¡œ ë°˜í™˜
        return pd.Series([True] * len(series), index=series.index)

# ë©”ì¸ ì•±
def main():
    # ìƒˆë¡œìš´ query_params ì‚¬ìš© (ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜)
    try:
        # URL íŒŒë¼ë¯¸í„° ì½ê¸° - ìƒˆë¡œìš´ ë°©ì‹
        default_initial = st.query_params.get("initial", "ì „ì²´")
        default_search = st.query_params.get("search", "")
        default_company = st.query_params.get("company", "")
    except:
        # íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        default_initial = "ì „ì²´"
        default_search = ""
        default_company = ""
    
    # AI ëª¨ë¸ ë¡œë“œ
    if 'model_loaded' not in st.session_state:
        st.info("ğŸ¤– AI ëª¨ë¸ì„ ì²˜ìŒ ë¡œë“œí•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
        model, tokenizer = download_and_load_models()
        st.session_state.model = model
        st.session_state.tokenizer = tokenizer
        st.session_state.model_loaded = True
        
        if model is not None:
            st.success("âœ… AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì´ì œ ì§€ëŠ¥í˜• ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        st.rerun()
    
    # ë°ì´í„° ë¡œë“œ
    df = load_data()
    
    if df.empty:
        st.error("âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì‚¬ì´ë“œë°” í•„í„°
    st.sidebar.header("ğŸ“‚ í•„í„° ì˜µì…˜")
    
    # ì´ˆì„± í•„í„°
    initials = ['ì „ì²´', 'ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
    selected_initial = st.sidebar.selectbox("ğŸ”¡ ì¢…ëª©ëª… ì´ˆì„±:", initials, 
                                          index=initials.index(default_initial) if default_initial in initials else 0)
    
    # í…ìŠ¤íŠ¸ ê²€ìƒ‰
    search_term = st.sidebar.text_input("ğŸ” ì¢…ëª©ëª… ë˜ëŠ” í‹°ì»¤ ê²€ìƒ‰", value=default_search)
    
    # ê³ ê¸‰ ê²€ìƒ‰ ì˜µì…˜
    with st.sidebar.expander("ğŸ”§ ê³ ê¸‰ ê²€ìƒ‰ ì˜µì…˜"):
        # ì•ˆì „í•œ ë²”ìœ„ ê³„ì‚°
        try:
            per_min = float(df["PER_ìµœê·¼"].min()) if df["PER_ìµœê·¼"].notna().any() else 0.0
            per_max = float(df["PER_ìµœê·¼"].max()) if df["PER_ìµœê·¼"].notna().any() else 50.0
            pbr_min = float(df["PBR_ìµœê·¼"].min()) if df["PBR_ìµœê·¼"].notna().any() else 0.0
            pbr_max = float(df["PBR_ìµœê·¼"].max()) if df["PBR_ìµœê·¼"].notna().any() else 10.0
            price_min = int(df["í˜„ì¬ê°€"].min()) if df["í˜„ì¬ê°€"].notna().any() else 1000
            price_max = int(df["í˜„ì¬ê°€"].max()) if df["í˜„ì¬ê°€"].notna().any() else 100000
        except:
            per_min, per_max = 0.0, 50.0
            pbr_min, pbr_max = 0.0, 10.0
            price_min, price_max = 1000, 100000
        
        # PER ë²”ìœ„ í•„í„°
        per_range = st.slider("PER ë²”ìœ„", per_min, per_max, (per_min, per_max))
        
        # PBR ë²”ìœ„ í•„í„°
        pbr_range = st.slider("PBR ë²”ìœ„", pbr_min, pbr_max, (pbr_min, pbr_max))
        
        # ì‹œê°€ì´ì•¡ ë²”ìœ„ (í˜„ì¬ê°€ ê¸°ì¤€)
        price_range = st.slider("ì£¼ê°€ ë²”ìœ„", price_min, price_max, (price_min, price_max))
    
    # ë°ì´í„° í•„í„°ë§
    filtered_df = df.copy()
    
    # ì´ˆì„± í•„í„° ì ìš©
    if selected_initial != "ì „ì²´":
        filtered_df = filtered_df[filtered_df["ì¢…ëª©ëª…"].apply(lambda x: get_initial(x[0]) == selected_initial)]
    
    # í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì ìš©
    if search_term:
        mask1 = filtered_df["ì¢…ëª©ëª…"].str.contains(search_term, case=False, na=False)
        mask2 = filtered_df["í‹°ì»¤"].str.contains(search_term, case=False, na=False)
        filtered_df = filtered_df[mask1 | mask2]
    
    # ê³ ê¸‰ í•„í„° ì ìš© - ì•ˆì „í•œ ë°©ì‹
    per_mask = safe_between_filter(filtered_df["PER_ìµœê·¼"], per_range[0], per_range[1])
    pbr_mask = safe_between_filter(filtered_df["PBR_ìµœê·¼"], pbr_range[0], pbr_range[1])
    price_mask = safe_between_filter(filtered_df["í˜„ì¬ê°€"], price_range[0], price_range[1])
    
    filtered_df = filtered_df[per_mask & pbr_mask & price_mask]
    
    ì¢…ëª©_list = filtered_df["ì¢…ëª©ëª…"].tolist()
    
    if not ì¢…ëª©_list:
        st.warning("âŒ ì¡°ê±´ì— ë§ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¢…ëª© ì„ íƒ
    if default_company and default_company in ì¢…ëª©_list:
        default_index = ì¢…ëª©_list.index(default_company)
    else:
        default_index = 0
    
    ì„ íƒí•œ_ì¢…ëª© = st.sidebar.selectbox("ğŸ“Œ ì¢…ëª© ì„ íƒ:", ì¢…ëª©_list, index=default_index)
    ì¢…ëª©_df = filtered_df[filtered_df["ì¢…ëª©ëª…"] == ì„ íƒí•œ_ì¢…ëª©].iloc[0]
    
    # URL ê³µìœ  ë§í¬ ìƒì„± - ìƒˆë¡œìš´ ë°©ì‹
    if st.sidebar.button("ğŸ”— í˜„ì¬ ì„¤ì • URLì— ì €ì¥"):
        st.query_params.initial = selected_initial
        st.query_params.search = search_term
        st.query_params.company = ì„ íƒí•œ_ì¢…ëª©
        st.sidebar.success("âœ… URLì— í˜„ì¬ ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ----------------------- ë©”ì¸ ì»¨í…ì¸  -----------------------
    st.title(f"ğŸ“Š {ì„ íƒí•œ_ì¢…ëª©} ({ì¢…ëª©_df['í‹°ì»¤']}) AI ë¦¬ì„œì¹˜ ë¶„ì„")
    
    # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
    if len(ì¢…ëª©_list) < len(df):
        st.info(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: {len(ì¢…ëª©_list)}ê°œ ì¢…ëª© (ì „ì²´ {len(df)}ê°œ ì¤‘)")
    
    # ì¬ë¬´ ì§€í‘œ
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("í˜„ì¬ê°€", f"{ì¢…ëª©_df['í˜„ì¬ê°€']:,.0f}ì›")
        st.metric("ROE (ìµœê·¼)", f"{ì¢…ëª©_df['ROE_ìµœê·¼']:.2f}%")
        st.metric("PER (ìµœê·¼)", f"{ì¢…ëª©_df['PER_ìµœê·¼']:.2f}")
        st.metric("PBR (ìµœê·¼)", f"{ì¢…ëª©_df['PBR_ìµœê·¼']:.2f}")
        st.metric("ë¶€ì±„ë¹„ìœ¨", f"{ì¢…ëª©_df['ë¶€ì±„ë¹„ìœ¨_ìµœê·¼']:.2f}%")
    
    with col2:
        if 'ìœ ë³´ìœ¨_ìµœê·¼' in ì¢…ëª©_df:
            st.metric("ìœ ë³´ìœ¨", f"{ì¢…ëª©_df['ìœ ë³´ìœ¨_ìµœê·¼']:.2f}%")
        if 'ë§¤ì¶œì•¡_ìµœê·¼' in ì¢…ëª©_df:
            st.metric("ë§¤ì¶œì•¡", f"{ì¢…ëª©_df['ë§¤ì¶œì•¡_ìµœê·¼']:,.0f}ì›")
        if 'ì˜ì—…ì´ìµ_ìµœê·¼' in ì¢…ëª©_df:
            st.metric("ì˜ì—…ì´ìµ", f"{ì¢…ëª©_df['ì˜ì—…ì´ìµ_ìµœê·¼']:,.0f}ì›")
        if 'ìˆœì´ìµ_ìµœê·¼' in ì¢…ëª©_df:
            st.metric("ìˆœì´ìµ", f"{ì¢…ëª©_df['ìˆœì´ìµ_ìµœê·¼']:,.0f}ì›")
    
    # ì£¼ê°€ ì°¨íŠ¸
    st.markdown("### ğŸ“ˆ ì£¼ê°€ ì¶”ì´")
    price_cols = [col for col in df.columns if col.isdigit() and len(col) == 8]
    if price_cols:
        try:
            price_series = ì¢…ëª©_df[price_cols].astype(float)
            price_series.index = pd.to_datetime(price_cols, format='%Y%m%d')
            chart_df = price_series.reset_index().rename(columns={'index': 'ë‚ ì§œ'})
            st.line_chart(chart_df.set_index("ë‚ ì§œ"))
        except:
            st.info("ì£¼ê°€ ì°¨íŠ¸ ë°ì´í„°ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë‰´ìŠ¤ ì„¹ì…˜
    st.markdown("### ğŸ“° ìµœê·¼ ë‰´ìŠ¤")
    if "ìµœì‹ ë‰´ìŠ¤" in ì¢…ëª©_df and isinstance(ì¢…ëª©_df["ìµœì‹ ë‰´ìŠ¤"], str) and ì¢…ëª©_df["ìµœì‹ ë‰´ìŠ¤"].strip():
        for i, link in enumerate(ì¢…ëª©_df["ìµœì‹ ë‰´ìŠ¤"].splitlines(), 1):
            if link.strip():
                st.markdown(f"{i}. [ë‰´ìŠ¤ ë§í¬]({link.strip()})")
    else:
        st.info("ìµœê·¼ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # AI ë¶„ì„ ì„¹ì…˜
    st.markdown("### ğŸ¤– AI ì¦ê¶Œ ë¶„ì„")
    
    # ë¯¸ë¦¬ ì •ì˜ëœ ì§ˆë¬¸ë“¤
    preset_questions = [
        "ì´ ì¢…ëª©ì˜ íˆ¬ì ë§¤ë ¥ë„ëŠ” ì–´ë–¤ê°€ìš”?",
        "PERê³¼ PBR ì§€í‘œë¥¼ ì–´ë–»ê²Œ í•´ì„í•´ì•¼ í•˜ë‚˜ìš”?",
        "í˜„ì¬ ì¬ë¬´ ìƒíƒœì˜ ê°•ì ê³¼ ì•½ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì´ ì¢…ëª©ì˜ ë¦¬ìŠ¤í¬ ìš”ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ë™ì¢… ì—…ê³„ ëŒ€ë¹„ ê²½ìŸë ¥ì€ ì–´ë–¤ê°€ìš”?"
    ]
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        user_question = st.text_input("ğŸ’¬ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:", 
                                    placeholder="ì˜ˆ: ì´ ì¢…ëª©ì˜ PERì´ ë†’ìœ¼ë©´ ì–´ë–¤ í•´ì„ì´ ê°€ëŠ¥í•´?")
    
    with col2:
        selected_preset = st.selectbox("ğŸ“‹ ë¯¸ë¦¬ ì •ì˜ëœ ì§ˆë¬¸:", ["ì§ì ‘ ì…ë ¥"] + preset_questions)
    
    if selected_preset != "ì§ì ‘ ì…ë ¥":
        user_question = selected_preset
    
    if user_question and st.button("ğŸ” AI ë¶„ì„ ìš”ì²­", type="primary"):
        if st.session_state.get('model') is not None:
            with st.spinner("ğŸ¤– AIê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                ai_response = generate_ai_response(
                    st.session_state.model, 
                    st.session_state.tokenizer, 
                    user_question, 
                    ì¢…ëª©_df
                )
            
            st.markdown("#### ğŸ¯ AI ë¶„ì„ ê²°ê³¼")
            st.markdown(ai_response)
            
            # ë¶„ì„ ê²°ê³¼ ì €ì¥ ì˜µì…˜
            if st.button("ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥"):
                analysis_text = f"""
# {ì„ íƒí•œ_ì¢…ëª©} AI ë¶„ì„ ê²°ê³¼

**ì§ˆë¬¸:** {user_question}

**AI ë¶„ì„:**
{ai_response}

**ê¸°ë³¸ ì •ë³´:**
- ì¢…ëª©ëª…: {ì¢…ëª©_df['ì¢…ëª©ëª…']}
- í‹°ì»¤: {ì¢…ëª©_df['í‹°ì»¤']}
- í˜„ì¬ê°€: {ì¢…ëª©_df['í˜„ì¬ê°€']:,.0f}ì›
- PER: {ì¢…ëª©_df['PER_ìµœê·¼']:.2f}
- PBR: {ì¢…ëª©_df['PBR_ìµœê·¼']:.2f}
- ROE: {ì¢…ëª©_df['ROE_ìµœê·¼']:.2f}%
"""
                st.download_button(
                    label="ğŸ“¥ ë¶„ì„ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ",
                    data=analysis_text,
                    file_name=f"{ì„ íƒí•œ_ì¢…ëª©}_AIë¶„ì„_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.md",
                    mime="text/markdown"
                )
        else:
            st.error("âŒ AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ì£¼ì„¸ìš”.")
    
    # ê²€ìƒ‰ í†µê³„
    with st.expander("ğŸ“Š ê²€ìƒ‰ í†µê³„"):
        st.write(f"**ì „ì²´ ì¢…ëª© ìˆ˜:** {len(df)}")
        st.write(f"**í•„í„°ë§ëœ ì¢…ëª© ìˆ˜:** {len(filtered_df)}")
        if len(filtered_df) > 0:
            st.write(f"**í‰ê·  PER:** {filtered_df['PER_ìµœê·¼'].mean():.2f}")
            st.write(f"**í‰ê·  PBR:** {filtered_df['PBR_ìµœê·¼'].mean():.2f}")
            st.write(f"**í‰ê·  ROE:** {filtered_df['ROE_ìµœê·¼'].mean():.2f}%")

if __name__ == "__main__":
    main()
