# kospi_research_app.py
import os
import json

# PyTorchμ™€ Streamlit νΈν™μ„± λ¬Έμ  ν•΄κ²° (λ°λ“μ‹ κ°€μ¥ λ¨Όμ € μ‹¤ν–‰)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

# PyTorch μ„ν¬νΈ λ° νΈν™μ„± μμ •
import torch
try:
    torch.classes.__path__ = [os.path.join(torch.__path__[0], 'classes')]
except Exception:
    try:
        if hasattr(torch, 'classes'):
            torch.classes.__path__._path = [os.path.join(torch.__path__[0], 'classes')]
    except Exception:
        pass

import streamlit as st
import pandas as pd
import sqlite3
import gdown
import zipfile
import tarfile
import torch.quantization
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig
import numpy as np
import gc

st.set_page_config(page_title="π“ KOSPI Analyst AI", layout="wide")

def cleanup_memory():
    """λ©”λ¨λ¦¬ μ •λ¦¬ ν•¨μ"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def fix_config_json(model_path):
    """config.json νμΌμ model_type ν‚¤ μμ •"""
    config_path = os.path.join(model_path, "config.json")
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # model_typeμ΄ μ—†μΌλ©΄ μ¶”κ°€
            if 'model_type' not in config:
                # μ²« λ²μ§Έ, λ‘ λ²μ§Έ μ½”λ“μ—μ„ Llama λ¨λΈμ„ μ‚¬μ©ν•λ―€λ΅
                config['model_type'] = 'llama'
                
                # μμ •λ config.json μ €μ¥
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(config, f, indent=2, ensure_ascii=False)
                
                st.info(f"β… config.jsonμ— model_type μ¶”κ°€: {config['model_type']}")
            else:
                st.info(f"β… config.jsonμ— model_type ν™•μΈ: {config['model_type']}")
                
        except Exception as e:
            st.warning(f"config.json μμ • μ‹¤ν¨: {e}")
    else:
        st.error(f"config.json νμΌμ΄ μ—†μµλ‹λ‹¤: {config_path}")

def find_best_checkpoint(base_path, preferred_checkpoint="checkpoint-200"):
    """μµμ μ μ²΄ν¬ν¬μΈνΈ κ²½λ΅ μ°ΎκΈ°"""
    # μ„ νΈν•λ” μ²΄ν¬ν¬μΈνΈ λ¨Όμ € ν™•μΈ
    preferred_path = os.path.join(base_path, preferred_checkpoint)
    if os.path.exists(preferred_path):
        adapter_config = os.path.join(preferred_path, "adapter_config.json")
        adapter_model = os.path.join(preferred_path, "adapter_model.safetensors")
        if not os.path.exists(adapter_model):
            adapter_model = os.path.join(preferred_path, "adapter_model.bin")
        
        if os.path.exists(adapter_config) and os.path.exists(adapter_model):
            return preferred_path, preferred_checkpoint
    
    # μ„ νΈν•λ” μ²΄ν¬ν¬μΈνΈκ°€ μ—†μΌλ©΄ λ‹¤λ¥Έ μ²΄ν¬ν¬μΈνΈ νƒμƒ‰
    checkpoint_dirs = []
    if os.path.exists(base_path):
        for item in os.listdir(base_path):
            if item.startswith("checkpoint-") and os.path.isdir(os.path.join(base_path, item)):
                checkpoint_dirs.append(item)
    
    # μ²΄ν¬ν¬μΈνΈ λ²νΈ μμΌλ΅ μ •λ ¬ (λ†’μ€ λ²νΈλ¶€ν„°)
    checkpoint_dirs.sort(key=lambda x: int(x.split("-")[1]), reverse=True)
    
    for checkpoint_dir in checkpoint_dirs:
        checkpoint_path = os.path.join(base_path, checkpoint_dir)
        adapter_config = os.path.join(checkpoint_path, "adapter_config.json")
        adapter_model = os.path.join(checkpoint_path, "adapter_model.safetensors")
        if not os.path.exists(adapter_model):
            adapter_model = os.path.join(checkpoint_path, "adapter_model.bin")
        
        if os.path.exists(adapter_config) and os.path.exists(adapter_model):
            return checkpoint_path, checkpoint_dir
    
    return None, None

@st.cache_resource
def download_and_load_models():
    """Google Driveμ—μ„ λ¨λΈ λ‹¤μ΄λ΅λ“ λ° CPU μµμ ν™” QLoRA λ΅λ“"""
    
    # Google Drive νμΌ ID
    base_model_id = "1CGpO7EO64hkUTU_eQQuZXbh-R84inkIc"
    qlora_adapter_id = "1l2F6a5HpmEmdOwTKOpu5UNRQG_jrXeW0"
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        cleanup_memory()
        
        # λ² μ΄μ¤ λ¨λΈ λ‹¤μ΄λ΅λ“
        status_text.text("π”„ λ² μ΄μ¤ λ¨λΈ λ‹¤μ΄λ΅λ“ μ¤‘... (1/9)")
        progress_bar.progress(10)
        
        if not os.path.exists("./base_model"):
            base_model_url = f"https://drive.google.com/uc?id={base_model_id}"
            gdown.download(base_model_url, "./my_base_model.tar.gz", quiet=False)
            
            with tarfile.open("./my_base_model.tar.gz", "r:gz") as tar:
                tar.extractall("./base_model/")
        
        # config.json νμΌ μμ • (model_type μ¶”κ°€)
        status_text.text("π”§ λ² μ΄μ¤ λ¨λΈ μ„¤μ • μμ • μ¤‘... (2/9)")
        progress_bar.progress(20)
        fix_config_json("./base_model")
        
        # QLoRA μ–΄λ‘ν„° λ‹¤μ΄λ΅λ“
        status_text.text("π”„ QLoRA μ–΄λ‘ν„° λ‹¤μ΄λ΅λ“ μ¤‘... (3/9)")
        progress_bar.progress(25)
        
        if not os.path.exists("./qlora_adapter"):
            qlora_url = f"https://drive.google.com/uc?id={qlora_adapter_id}"
            gdown.download(qlora_url, "./qlora_results.zip", quiet=False)
            
            with zipfile.ZipFile("./qlora_results.zip", 'r') as zip_ref:
                zip_ref.extractall("./qlora_adapter/")
        
        # μ²΄ν¬ν¬μΈνΈ κ²½λ΅ ν™•μΈ
        status_text.text("π”§ QLoRA μ²΄ν¬ν¬μΈνΈ ν™•μΈ μ¤‘... (4/9)")
        progress_bar.progress(35)
        
        # checkpoint-200μ„ μ°μ„ μ μΌλ΅ μ°ΎκΈ°
        adapter_path, checkpoint_name = find_best_checkpoint("./qlora_adapter", "checkpoint-200")
        
        if adapter_path is None:
            st.error("β QLoRA μ²΄ν¬ν¬μΈνΈλ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤.")
            return None, None
        
        st.info(f"β… μ‚¬μ©ν•  μ²΄ν¬ν¬μΈνΈ: {checkpoint_name}")
        
        # μ²΄ν¬ν¬μΈνΈ μ •λ³΄ ν‘μ‹
        try:
            with open(os.path.join(adapter_path, "adapter_config.json"), 'r') as f:
                adapter_config = json.load(f)
            st.info(f"π“‹ LoRA μ„¤μ •: r={adapter_config.get('r', 'N/A')}, alpha={adapter_config.get('lora_alpha', 'N/A')}")
        except:
            pass
        
        # ν† ν¬λ‚μ΄μ € λ΅λ“ (λ² μ΄μ¤ λ¨λΈμ—μ„)
        status_text.text("π“ ν† ν¬λ‚μ΄μ € λ΅λ“ μ¤‘... (5/9)")
        progress_bar.progress(45)
        
        tokenizer = AutoTokenizer.from_pretrained(
            "./base_model",
            trust_remote_code=True,
            use_fast=False,  # CPU ν™κ²½μ—μ„ μ•μ •μ„± μ°μ„ 
            padding_side="right"
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
        
        # λ² μ΄μ¤ λ¨λΈ λ΅λ“ (CPU μµμ ν™”)
        status_text.text("π§  λ² μ΄μ¤ λ¨λΈ λ΅λ“ μ¤‘ (CPU μµμ ν™”)... (6/9)")
        progress_bar.progress(55)
        
        base_model = AutoModelForCausalLM.from_pretrained(
            "./base_model",
            torch_dtype=torch.float32,  # CPUμ—μ„λ” float32 μ‚¬μ©
            device_map="cpu",
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            use_cache=False,  # λ©”λ¨λ¦¬ μ μ•½
            torch_compile=False  # CPUμ—μ„λ” μ»΄νμΌ λΉ„ν™μ„±ν™”
        )
        
        cleanup_memory()
        
        # QLoRA μ–΄λ‘ν„° μ„¤μ • ν™•μΈ
        status_text.text("π”§ QLoRA μ–΄λ‘ν„° μ„¤μ • ν™•μΈ μ¤‘... (7/9)")
        progress_bar.progress(65)
        
        try:
            peft_config = PeftConfig.from_pretrained(adapter_path)
            st.info(f"β… PEFT μ„¤μ •: {peft_config.task_type}, target_modules={len(peft_config.target_modules)}κ°")
        except Exception as e:
            st.warning(f"PeftConfig λ΅λ“ μ‹¤ν¨: {e}")
        
        # QLoRA μ–΄λ‘ν„° μ μ©
        status_text.text(f"π”§ {checkpoint_name} μ–΄λ‘ν„° μ μ© μ¤‘... (8/9)")
        progress_bar.progress(75)
        
        model = PeftModel.from_pretrained(
            base_model, 
            adapter_path,
            torch_dtype=torch.float32,
            is_trainable=False  # μ¶”λ΅  μ „μ©
        )
        
        # λ¨λΈμ„ ν‰κ°€ λ¨λ“λ΅ μ„¤μ •
        model.eval()
        
        cleanup_memory()
        
        # CPU λ™μ  μ–‘μν™” μ μ©
        status_text.text("β΅ CPU λ™μ  μ–‘μν™” μ μ© μ¤‘... (9/9)")
        progress_bar.progress(85)
        
        try:
            with torch.no_grad():
                # Linear λ μ΄μ–΄λ§ INT8λ΅ μ–‘μν™”
                quantized_model = torch.quantization.quantize_dynamic(
                    model,
                    {torch.nn.Linear},
                    dtype=torch.qint8,
                    inplace=False
                )
            model = quantized_model
            st.success("β… CPU λ™μ  μ–‘μν™” μ μ© μ™„λ£!")
        except Exception as e:
            st.warning(f"λ™μ  μ–‘μν™” μ‹¤ν¨, μ›λ³Έ λ¨λΈ μ‚¬μ©: {e}")
        
        progress_bar.progress(90)
        
        # λ¨λΈ μ •λ³΄ ν‘μ‹
        def get_model_size_safe(model):
            try:
                param_size = sum(p.numel() * p.element_size() for p in model.parameters())
                buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
                return (param_size + buffer_size) / 1024 / 1024
            except:
                return 0
        
        model_size = get_model_size_safe(model)
        
        # μ–΄λ‘ν„° μ •λ³΄ ν‘μ‹
        if hasattr(model, 'peft_config') and model.peft_config:
            config = list(model.peft_config.values())[0]
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("λ¨λΈ ν¬κΈ°", f"{model_size:.1f} MB")
            with col2:
                st.metric("μ²΄ν¬ν¬μΈνΈ", checkpoint_name.split("-")[1])
            with col3:
                st.metric("LoRA Rank", f"{config.r}")
            with col4:
                st.metric("LoRA Alpha", f"{config.lora_alpha}")
        
        progress_bar.progress(100)
        status_text.text("β… QLoRA λ¨λΈ λ΅λ“ μ™„λ£!")
        st.success(f"β΅ CPU μµμ ν™”λ {checkpoint_name} QLoRA λ¨λΈμ΄ μ„±κ³µμ μΌλ΅ λ΅λ“λμ—μµλ‹λ‹¤!")
        
        # μ„μ‹ νμΌ μ •λ¦¬
        try:
            for temp_file in ["./my_base_model.tar.gz", "./qlora_results.zip"]:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
        except:
            pass
        
        cleanup_memory()
        
        # UI μ •λ¦¬
        progress_bar.empty()
        status_text.empty()
        
        return model, tokenizer
        
    except Exception as e:
        st.error(f"β λ¨λΈ λ΅λ“ μ‹¤ν¨: {e}")
        progress_bar.empty()
        status_text.empty()
        cleanup_memory()
        return None, None

# λ‚λ¨Έμ§€ μ½”λ“λ” μ΄μ „κ³Ό λ™μΌ...
