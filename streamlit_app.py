import os
import streamlit as st
from transformers import pipeline
import glob
import asyncio
import torch

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["STREAMLIT_SERVER_ENABLE_STATIC_SERVE"] = "false"
os.environ["HF_HOME"] = "/tmp/hf_home"
os.environ["TRANSFORMERS_CACHE"] = "/tmp/hf_home/models"
os.environ["HUGGINGFACE_HUB_CACHE"] = "/tmp/hf_home/hub"
os.environ["STREAMLIT_CONFIG_DIR"] = "/tmp/.streamlit"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTHONWARNINGS"] = "ignore::FutureWarning"
os.environ["HOME"] = "/tmp"  # ì¤‘ìš”: HOME í™˜ê²½ë³€ìˆ˜ë„ ì„¤ì •

# ë””ë ‰í† ë¦¬ ìƒì„± ë° ê¶Œí•œ ì„¤ì •
cache_dirs = [
    "/tmp/hf_home",
    "/tmp/hf_home/models",
    "/tmp/hf_home/hub",
    "/tmp/.streamlit"
]

def fix_permissions(path):
    for root, dirs, files in os.walk(path):
        for d in dirs:
            os.chmod(os.path.join(root, d), 0o777)
        for f in files:
            os.chmod(os.path.join(root, f), 0o777)

for dir_path in cache_dirs:
    os.makedirs(dir_path, exist_ok=True)
    os.chmod(dir_path, 0o777)
    fix_permissions(dir_path)

# ë½ íŒŒì¼ ì‚­ì œ
lock_files = glob.glob("/tmp/hf_home/**/*.lock", recursive=True)
for lock_file in lock_files:
    try:
        os.remove(lock_file)
    except Exception:
        pass

# PyTorch JIT ì„¤ì • (ë¬¸ì œ ìˆë‹¤ë©´ ì£¼ì„ ì²˜ë¦¬)
try:
    torch._C._jit_set_profiling_executor(False)
    torch._C._jit_set_profiling_mode(False)
except Exception:
    pass

def ensure_event_loop():
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        asyncio.set_event_loop(asyncio.new_event_loop())

@st.cache_resource
def load_model():
    ensure_event_loop()
    return pipeline(
        'text-generation',
        model='TinyLlama/TinyLlama-1.1B-Chat-v1.0',
        device_map="auto",
        torch_dtype="auto",
        cache_dir="/tmp/hf_home/models"
    )

def main():
    st.title("ğŸš€ TinyLlama í…ŒìŠ¤íŠ¸")
    prompt = st.text_input("í”„ë¡¬í”„íŠ¸ ì…ë ¥:")
    
    if st.button("ì‹¤í–‰"):
        try:
            pipe = load_model()
            output = pipe(prompt, max_new_tokens=200)[0]['generated_text']
            st.write(output)
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            st.info("""
            **ì¶”ê°€ ì¡°ì¹˜ ë°©ë²•**
            1. í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
            2. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ ì‹¤í–‰:
               ```
               rm -rf /tmp/hf_home/**/*.lock
               ```
            3. ëª¨ë¸ ìºì‹œ ìˆ˜ë™ ì‚­ì œ:
               ```
               rm -rf /tmp/hf_home/models
               ```
            """)

if __name__ == "__main__":
    main()
