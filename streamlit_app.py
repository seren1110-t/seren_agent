# kospi_research_app.py
import os
import json

# PyTorchì™€ Streamlit í˜¸í™˜ì„± ë¬¸ì œ í•´ê²° (ë°˜ë“œì‹œ ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

# PyTorch ì„í¬íŠ¸ ë° í˜¸í™˜ì„± ìˆ˜ì •
import torch
try:
    torch.classes.__path__ = [os.path.join(torch.__path__[0], 'classes')]
except Exception:
    try:
        if hasattr(torch, 'classes'):
            torch.classes.__path__._path = [os.path.join(torch.__path__[0], 'classes')]
    except Exception:
        pass

import streamlit as st
import pandas as pd
import sqlite3
import gdown
import zipfile
import tarfile
import torch.quantization
import transformers
from transformers.models.auto import AutoTokenizer, AutoModelForCausalLM
import numpy as np
import gc

st.set_page_config(page_title="ğŸ“ˆ KOSPI Analyst AI", layout="wide")

def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def safe_load_tokenizer(model_path):
    """ì•ˆì „í•œ í† í¬ë‚˜ì´ì € ë¡œë“œ - 'bool' object has no attribute 'pad_token' ì˜¤ë¥˜ í•´ê²°"""
    try:
        # ë°©ë²• 1: ê¸°ë³¸ ë¡œë“œ ì‹œë„
        st.info("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œë„ ì¤‘...")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            use_fast=False,
            padding_side="right"
        )
        
        # pad_token ì•ˆì „í•˜ê²Œ ì„¤ì •
        if not hasattr(tokenizer, 'pad_token') or tokenizer.pad_token is None:
            st.info("ğŸ”§ pad_token ì„¤ì • ì¤‘...")
            if hasattr(tokenizer, 'eos_token') and tokenizer.eos_token is not None:
                tokenizer.pad_token = tokenizer.eos_token
                if hasattr(tokenizer, 'eos_token_id') and tokenizer.eos_token_id is not None:
                    tokenizer.pad_token_id = tokenizer.eos_token_id
                else:
                    tokenizer.pad_token_id = 2  # ê¸°ë³¸ê°’
            else:
                # eos_tokenì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ì¶”ê°€
                tokenizer.add_special_tokens({'pad_token': '[PAD]'})
                tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')
        
        # í† í¬ë‚˜ì´ì € ê²€ì¦
        if hasattr(tokenizer, 'pad_token') and tokenizer.pad_token is not None:
            st.success(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ! pad_token: {tokenizer.pad_token}")
            return tokenizer, None
        else:
            raise ValueError("pad_token ì„¤ì • ì‹¤íŒ¨")
        
    except Exception as e:
        st.warning(f"ê¸°ë³¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 2: ëŒ€ì•ˆ ë¡œë“œ ë°©ì‹
        try:
            st.info("ğŸ”„ ëŒ€ì•ˆ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œë„...")
            
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                use_fast=True,  # fast tokenizer ì‹œë„
                padding_side="right",
                add_eos_token=True
            )
            
            # ê°•ì œ pad_token ì„¤ì •
            tokenizer.pad_token = "</s>"  # ê¸°ë³¸ EOS í† í°
            tokenizer.pad_token_id = 2    # ê¸°ë³¸ EOS í† í° ID
            
            st.success("âœ… ëŒ€ì•ˆ í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ!")
            return tokenizer, None
            
        except Exception as e2:
            st.error(f"ëŒ€ì•ˆ í† í¬ë‚˜ì´ì € ë¡œë“œë„ ì‹¤íŒ¨: {e2}")
            
            # ë°©ë²• 3: ìµœì†Œí•œì˜ í† í¬ë‚˜ì´ì € ìƒì„±
            try:
                st.info("ğŸ†˜ ìµœì†Œí•œì˜ í† í¬ë‚˜ì´ì € ìƒì„± ì‹œë„...")
                
                # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í† í¬ë‚˜ì´ì € ìƒì„±
                from transformers import GPT2Tokenizer
                tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
                
                # í•œêµ­ì–´ ì§€ì›ì„ ìœ„í•œ ìµœì†Œ ì„¤ì •
                tokenizer.pad_token = tokenizer.eos_token
                tokenizer.pad_token_id = tokenizer.eos_token_id
                
                st.warning("âš ï¸ ê¸°ë³¸ í† í¬ë‚˜ì´ì €ë¡œ ëŒ€ì²´ë¨ (ì„±ëŠ¥ ì œí•œ)")
                return tokenizer, None
                
            except Exception as e3:
                return None, f"ëª¨ë“  í† í¬ë‚˜ì´ì € ë¡œë“œ ë°©ë²• ì‹¤íŒ¨: {e3}"

def verify_zip_file(file_path):
    """ZIP íŒŒì¼ ê²€ì¦"""
    try:
        if not os.path.exists(file_path):
            return False, "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        
        file_size = os.path.getsize(file_path)
        if file_size < 1000:  # 1KB ë¯¸ë§Œ
            return False, f"íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤ ({file_size} bytes). ì˜¤ë¥˜ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
        
        # ZIP íŒŒì¼ ê²€ì¦
        with zipfile.ZipFile(file_path, 'r') as zip_ref:
            file_list = zip_ref.namelist()
            if len(file_list) == 0:
                return False, "ë¹ˆ ZIP íŒŒì¼ì…ë‹ˆë‹¤."
            
            # í•„ìš”í•œ íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
            required_files = ['base_model/', 'cpu_deployment_config.json']
            found_files = []
            for required in required_files:
                for file in file_list:
                    if required in file:
                        found_files.append(required)
                        break
            
            if len(found_files) < len(required_files):
                return False, f"í•„ìš”í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì°¾ì€ íŒŒì¼: {found_files}"
        
        return True, f"ìœ íš¨í•œ ZIP íŒŒì¼ì…ë‹ˆë‹¤. í¬í•¨ëœ íŒŒì¼: {len(file_list)}ê°œ"
        
    except zipfile.BadZipFile:
        # íŒŒì¼ ë‚´ìš© í™•ì¸ (HTML í˜ì´ì§€ì¸ì§€ ì²´í¬)
        try:
            with open(file_path, 'rb') as f:
                first_bytes = f.read(200)
                if b'<html' in first_bytes.lower() or b'<!doctype' in first_bytes.lower():
                    return False, "HTML í˜ì´ì§€ê°€ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. Google Drive í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
                else:
                    return False, f"ìœ íš¨í•˜ì§€ ì•Šì€ ZIP íŒŒì¼ì…ë‹ˆë‹¤. ì²« 200ë°”ì´íŠ¸: {first_bytes[:100]}..."
        except Exception as e:
            return False, f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}"
    except Exception as e:
        return False, f"ZIP ê²€ì¦ ì˜¤ë¥˜: {e}"

def download_with_verification(file_id, output_path, max_retries=3):
    """ê²€ì¦ì„ í¬í•¨í•œ ë‹¤ìš´ë¡œë“œ"""
    download_methods = [
        f"https://drive.google.com/uc?id={file_id}&confirm=t",
        f"https://drive.google.com/uc?id={file_id}",
        f"https://drive.google.com/uc?export=download&id={file_id}",
    ]
    
    for attempt in range(max_retries):
        st.info(f"ë‹¤ìš´ë¡œë“œ ì‹œë„ {attempt + 1}/{max_retries}")
        
        # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
        if os.path.exists(output_path):
            os.remove(output_path)
        
        for i, url in enumerate(download_methods):
            try:
                st.info(f"ë°©ë²• {i+1}: {url[:50]}...")
                gdown.download(url, output_path, quiet=False)
                
                # íŒŒì¼ ê²€ì¦
                is_valid, message = verify_zip_file(output_path)
                st.info(f"ê²€ì¦ ê²°ê³¼: {message}")
                
                if is_valid:
                    st.success("âœ… ìœ íš¨í•œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
                    return True
                else:
                    st.warning(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {message}")
                    continue
                    
            except Exception as e:
                st.warning(f"ë‹¤ìš´ë¡œë“œ ë°©ë²• {i+1} ì‹¤íŒ¨: {e}")
                continue
        
        st.warning(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨. ì ì‹œ í›„ ì¬ì‹œë„...")
        
    return False

def manual_download_guide(file_id, output_path):
    """ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ì•ˆë‚´"""
    st.error("ğŸš« ìë™ ë‹¤ìš´ë¡œë“œê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    with st.expander("ğŸ“‹ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°©ë²•", expanded=True):
        st.markdown(f"""
        **Google Drive í• ë‹¹ëŸ‰ ìš°íšŒ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ:**
        
        1. **ë¸Œë¼ìš°ì €ì—ì„œ ë‹¤ìš´ë¡œë“œ:**
           - [íŒŒì¼ ë§í¬](https://drive.google.com/file/d/{file_id}/view) í´ë¦­
           - "ë‚´ ë“œë¼ì´ë¸Œì— ì¶”ê°€" ë²„íŠ¼ í´ë¦­
           - ìƒˆ í´ë” ìƒì„± í›„ ë°”ë¡œê°€ê¸° ì¶”ê°€
           - í´ë” ì „ì²´ë¥¼ ë‹¤ìš´ë¡œë“œ (ZIPìœ¼ë¡œ ì••ì¶•ë¨)
           
        2. **íŒŒì¼ ì €ì¥ ìœ„ì¹˜:**
           - ë‹¤ìš´ë¡œë“œí•œ ZIP íŒŒì¼ì„ `{output_path}` ê²½ë¡œì— ì €ì¥
           
        3. **íŒŒì¼ ê²€ì¦:**
           - ZIP íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ì—´ë¦¬ëŠ”ì§€ í™•ì¸
           - íŒŒì¼ í¬ê¸°ê°€ 10MB ì´ìƒì¸ì§€ í™•ì¸
        """)
    
    # íŒŒì¼ í™•ì¸ ë²„íŠ¼
    if st.button("âœ… ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ", type="primary"):
        is_valid, message = verify_zip_file(output_path)
        if is_valid:
            st.success("íŒŒì¼ ê²€ì¦ ì™„ë£Œ! í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ê³„ì†í•˜ì„¸ìš”.")
            st.rerun()
        else:
            st.error(f"íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {message}")

@st.cache_resource
def download_and_load_models():
    """Google Driveì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° CPU ìµœì í™” ëª¨ë¸ ë¡œë“œ (í† í¬ë‚˜ì´ì € ì˜¤ë¥˜ ìˆ˜ì •)"""
    
    saved_model_id = "1kQs4co-fO5JOTaAQ6Hn8S0s4fwUh6qyo"
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        cleanup_memory()
        
        model_dir = "./koalpaca_streamlit_model"
        zip_path = "./koalpaca_streamlit_model.zip"
        
        if not os.path.exists(model_dir):
            status_text.text("ğŸ”„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê²€ì¦ ì¤‘... (1/6)")
            progress_bar.progress(15)
            
            # ê²€ì¦ì„ í¬í•¨í•œ ë‹¤ìš´ë¡œë“œ
            if download_with_verification(saved_model_id, zip_path):
                # ZIP íŒŒì¼ ì••ì¶• í•´ì œ
                status_text.text("ğŸ“¦ ì••ì¶• í•´ì œ ì¤‘...")
                progress_bar.progress(25)
                
                try:
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall("./")
                    st.success("âœ… ì••ì¶• í•´ì œ ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
                    return None, None
            else:
                # ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ì•ˆë‚´
                manual_download_guide(saved_model_id, zip_path)
                st.stop()
        
        # ì„¤ì • ì •ë³´ ë¡œë“œ
        status_text.text("ğŸ”§ ëª¨ë¸ ì„¤ì • ì •ë³´ í™•ì¸ ì¤‘... (2/6)")
        progress_bar.progress(35)
        
        config_path = os.path.join(model_dir, "cpu_deployment_config.json")
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_info = json.load(f)
            st.info(f"âœ… ëª¨ë¸ ì •ë³´: {config_info.get('model_type', 'Unknown')}")
            st.info(f"ğŸ“‹ ìš©ë„: {config_info.get('purpose', 'Unknown')}")
            st.info(f"ğŸ”§ ìµœì í™”: CPU + {config_info.get('quantization_method', 'Unknown')}")
        else:
            st.warning("âš ï¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì•ˆì „í•œ í† í¬ë‚˜ì´ì € ë¡œë“œ
        status_text.text("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘... (3/6)")
        progress_bar.progress(50)
        
        tokenizer, error = safe_load_tokenizer(model_dir)
        if tokenizer is None:
            st.error(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {error}")
            return None, None
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        status_text.text("ğŸ§  ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘... (4/6)")
        progress_bar.progress(65)
        
        base_model_path = os.path.join(model_dir, "base_model")
        
        if os.path.exists(base_model_path):
            try:
                model = AutoModelForCausalLM.from_pretrained(
                    base_model_path,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    low_cpu_mem_usage=True,
                    trust_remote_code=True,
                    use_cache=False,
                    torch_compile=False
                )
                st.success("âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            except Exception as e:
                st.error(f"âŒ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return None, None
        else:
            st.error(f"âŒ ë² ì´ìŠ¤ ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_model_path}")
            return None, None
        
        cleanup_memory()
        model.eval()
        
        # ì–‘ìí™” ì ìš©
        status_text.text("âš¡ ì–‘ìí™” ì ìš© ì¤‘... (5/6)")
        progress_bar.progress(80)
        
        quantized_model_path = os.path.join(model_dir, "cpu_quantized_model.pt")
        
        if os.path.exists(quantized_model_path):
            try:
                checkpoint = torch.load(quantized_model_path, map_location='cpu')
                if 'quantization_info' in checkpoint:
                    quant_info = checkpoint['quantization_info']
                    st.success(f"âœ… ì €ì¥ëœ ì–‘ìí™” ëª¨ë¸ ë°œê²¬: {quant_info['method']}")
                    if 'model_state_dict' in checkpoint:
                        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                        st.info("ğŸ“¦ ì €ì¥ëœ ì–‘ìí™” ê°€ì¤‘ì¹˜ ì ìš© ì™„ë£Œ")
            except Exception as e:
                st.warning(f"ì €ì¥ëœ ì–‘ìí™” ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                # ë™ì  ì–‘ìí™” ì ìš©
                try:
                    with torch.no_grad():
                        quantized_model = torch.quantization.quantize_dynamic(
                            model,
                            {torch.nn.Linear},
                            dtype=torch.qint8,
                            inplace=False
                        )
                    model = quantized_model
                    st.success("âœ… CPU ë™ì  ì–‘ìí™” ì ìš© ì™„ë£Œ!")
                except Exception as qe:
                    st.warning(f"ë™ì  ì–‘ìí™” ì‹¤íŒ¨, ì›ë³¸ ëª¨ë¸ ì‚¬ìš©: {qe}")
        else:
            # ë™ì  ì–‘ìí™” ì ìš©
            try:
                with torch.no_grad():
                    quantized_model = torch.quantization.quantize_dynamic(
                        model,
                        {torch.nn.Linear},
                        dtype=torch.qint8,
                        inplace=False
                    )
                model = quantized_model
                st.success("âœ… CPU ë™ì  ì–‘ìí™” ì ìš© ì™„ë£Œ!")
            except Exception as e:
                st.warning(f"ë™ì  ì–‘ìí™” ì‹¤íŒ¨, ì›ë³¸ ëª¨ë¸ ì‚¬ìš©: {e}")
        
        # ëª¨ë¸ ì •ë³´ í‘œì‹œ
        status_text.text("ğŸ“Š ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘... (6/6)")
        progress_bar.progress(95)
        
        def get_model_size_safe(model):
            try:
                param_size = sum(p.numel() * p.element_size() for p in model.parameters())
                buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
                return (param_size + buffer_size) / 1024 / 1024
            except:
                return 0
        
        model_size = get_model_size_safe(model)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ëª¨ë¸ í¬ê¸°", f"{model_size:.1f} MB")
        with col2:
            st.metric("ëª¨ë¸ íƒ€ì…", "KoAlpaca-Polyglot-5.8B")
        with col3:
            st.metric("ìµœì í™”", "CPU + ì–‘ìí™”")
        
        progress_bar.progress(100)
        status_text.text("âœ… CPU ìµœì í™” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        st.success("âš¡ CPU ìµœì í™”ëœ KoAlpaca ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            if os.path.exists(zip_path):
                os.remove(zip_path)
        except:
            pass
        
        cleanup_memory()
        
        # UI ì •ë¦¬
        progress_bar.empty()
        status_text.empty()
        
        return model, tokenizer
        
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        progress_bar.empty()
        status_text.empty()
        cleanup_memory()
        return None, None

def generate_response(model, tokenizer, prompt, max_new_tokens=512):
    """KoAlpaca ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„± (í† í¬ë‚˜ì´ì € ì˜¤ë¥˜ ë°©ì§€)"""
    try:
        # KoAlpaca í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì ìš©
        formatted_prompt = f"### ì§ˆë¬¸: {prompt}\n\n### ë‹µë³€:"
        
        # ì•ˆì „í•œ í† í¬ë‚˜ì´ì§•
        try:
            inputs = tokenizer(
                formatted_prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512,
                return_token_type_ids=False
            )
        except Exception as e:
            st.warning(f"í† í¬ë‚˜ì´ì§• ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ í† í¬ë‚˜ì´ì§• ì‹œë„
            inputs = tokenizer.encode(formatted_prompt, return_tensors="pt")
            inputs = {"input_ids": inputs}
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=getattr(tokenizer, 'pad_token_id', 2),
                eos_token_id=2,
                use_cache=True,
                repetition_penalty=1.1,
            )
        
        # ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œê±°)
        if 'input_ids' in inputs:
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs['input_ids'], outputs)
            ]
        else:
            generated_ids = outputs
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response.strip()
        
    except Exception as e:
        st.error(f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def create_sample_data():
    """ìƒ˜í”Œ KOSPI ë°ì´í„° ìƒì„±"""
    companies = [
        "ì‚¼ì„±ì „ì", "SKí•˜ì´ë‹‰ìŠ¤", "NAVER", "ì¹´ì¹´ì˜¤", "LGí™”í•™",
        "ì‚¼ì„±SDI", "í˜„ëŒ€ì°¨", "ê¸°ì•„", "POSCOí™€ë”©ìŠ¤", "KBê¸ˆìœµ"
    ]
    
    data = []
    for company in companies:
        data.append({
            "íšŒì‚¬ëª…": company,
            "í˜„ì¬ê°€": np.random.randint(50000, 500000),
            "ì „ì¼ëŒ€ë¹„": np.random.randint(-10000, 10000),
            "ë“±ë½ë¥ ": round(np.random.uniform(-5.0, 5.0), 2),
            "ê±°ë˜ëŸ‰": np.random.randint(100000, 10000000),
            "ì‹œê°€ì´ì•¡": np.random.randint(1000000, 100000000)
        })
    
    return pd.DataFrame(data)

def load_financial_data():
    """ê¸ˆìœµ ë°ì´í„° ë¡œë“œ (SQLite ë°ì´í„°ë² ì´ìŠ¤ ì‹œë®¬ë ˆì´ì…˜)"""
    try:
        # ì„ì‹œ SQLite ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
        conn = sqlite3.connect(':memory:')
        
        # ìƒ˜í”Œ ì¬ë¬´ ë°ì´í„° ìƒì„±
        financial_data = {
            'íšŒì‚¬ëª…': ['ì‚¼ì„±ì „ì', 'SKí•˜ì´ë‹‰ìŠ¤', 'NAVER', 'ì¹´ì¹´ì˜¤', 'LGí™”í•™'],
            'ë§¤ì¶œì•¡': [279000, 44000, 8800, 6800, 44000],
            'ì˜ì—…ì´ìµ': [43000, 8900, 1400, 500, 3200],
            'ë‹¹ê¸°ìˆœì´ìµ': [26900, 7300, 1200, 400, 2800],
            'ë¶€ì±„ë¹„ìœ¨': [15.2, 23.1, 8.9, 12.4, 67.8],
            'ROE': [9.8, 12.4, 8.7, 2.1, 7.9]
        }
        
        df = pd.DataFrame(financial_data)
        df.to_sql('financial_data', conn, index=False)
        
        # ë°ì´í„° ì¡°íšŒ
        result = pd.read_sql_query("SELECT * FROM financial_data", conn)
        conn.close()
        
        return result
    except Exception as e:
        st.error(f"ì¬ë¬´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def load_market_news():
    """ì‹œì¥ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ"""
    news_data = [
        {
            "ì œëª©": "ì‚¼ì„±ì „ì, 3ë¶„ê¸° ì‹¤ì  ì˜ˆìƒì¹˜ ìƒíšŒ",
            "ë‚´ìš©": "ì‚¼ì„±ì „ìê°€ 3ë¶„ê¸° ì˜ì—…ì´ìµì´ ì‹œì¥ ì˜ˆìƒì¹˜ë¥¼ ìƒíšŒí–ˆë‹¤ê³  ë°œí‘œí–ˆìŠµë‹ˆë‹¤.",
            "ë‚ ì§œ": "2025-06-08",
            "ì¹´í…Œê³ ë¦¬": "ì‹¤ì "
        },
        {
            "ì œëª©": "SKí•˜ì´ë‹‰ìŠ¤, AI ë©”ëª¨ë¦¬ ìˆ˜ìš” ê¸‰ì¦",
            "ë‚´ìš©": "AI ë°˜ë„ì²´ ìˆ˜ìš” ì¦ê°€ë¡œ SKí•˜ì´ë‹‰ìŠ¤ì˜ HBM ë©”ëª¨ë¦¬ ì£¼ë¬¸ì´ ê¸‰ì¦í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "ë‚ ì§œ": "2025-06-07",
            "ì¹´í…Œê³ ë¦¬": "ì‚°ì—…ë™í–¥"
        },
        {
            "ì œëª©": "KOSPI, ì™¸êµ­ì¸ ë§¤ìˆ˜ì„¸ë¡œ ìƒìŠ¹",
            "ë‚´ìš©": "ì™¸êµ­ì¸ íˆ¬ììë“¤ì˜ ë§¤ìˆ˜ì„¸ê°€ ì´ì–´ì§€ë©° KOSPIê°€ ìƒìŠ¹ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.",
            "ë‚ ì§œ": "2025-06-06",
            "ì¹´í…Œê³ ë¦¬": "ì‹œì¥ë™í–¥"
        }
    ]
    
    return pd.DataFrame(news_data)

def main():
    st.title("ğŸ“ˆ KOSPI Analyst AI")
    st.markdown("**KoAlpaca-Polyglot-5.8B ê¸°ë°˜ í•œêµ­ ì£¼ì‹ ë¶„ì„ AI**")
    
    # ëª¨ë¸ ë¡œë“œ
    with st.spinner("ğŸ”„ AI ëª¨ë¸ ë¡œë”© ì¤‘..."):
        model, tokenizer = download_and_load_models()
    
    if model is None or tokenizer is None:
        st.error("âŒ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ì£¼ì„¸ìš”.")
        return
    
    # ì‚¬ì´ë“œë°”
    st.sidebar.header("ğŸ“Š KOSPI ë°ì´í„°")
    
    # ì‹¤ì‹œê°„ ë°ì´í„° í‘œì‹œ
    df = create_sample_data()
    st.sidebar.dataframe(df, height=300)
    
    # ì¬ë¬´ ë°ì´í„° í‘œì‹œ
    st.sidebar.header("ğŸ’° ì¬ë¬´ ë°ì´í„°")
    financial_df = load_financial_data()
    if not financial_df.empty:
        st.sidebar.dataframe(financial_df, height=200)
    
    # ì‹œì¥ ë‰´ìŠ¤ í‘œì‹œ
    st.sidebar.header("ğŸ“° ì‹œì¥ ë‰´ìŠ¤")
    news_df = load_market_news()
    for _, news in news_df.iterrows():
        with st.sidebar.expander(f"ğŸ“° {news['ì œëª©'][:20]}..."):
            st.write(f"**ë‚ ì§œ:** {news['ë‚ ì§œ']}")
            st.write(f"**ì¹´í…Œê³ ë¦¬:** {news['ì¹´í…Œê³ ë¦¬']}")
            st.write(f"**ë‚´ìš©:** {news['ë‚´ìš©']}")
    
    # ë©”ì¸ ì»¨í…ì¸ 
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ’¬ AI ë¶„ì„", "ğŸ“ˆ ì°¨íŠ¸ ë¶„ì„", "ğŸ“‹ ë³´ê³ ì„œ ìƒì„±", "ğŸ“Š ë°ì´í„° ë¶„ì„"])
    
    with tab1:
        st.header("ğŸ’¬ AI ì£¼ì‹ ë¶„ì„")
        
        # ì§ˆë¬¸ ì…ë ¥
        user_question = st.text_area(
            "ğŸ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ì˜ˆ: ì‚¼ì„±ì „ìì˜ íˆ¬ì ì „ë§ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.",
            height=100
        )
        
        col1, col2 = st.columns([1, 4])
        with col1:
            if st.button("ğŸ¤– ë¶„ì„ ì‹œì‘", type="primary"):
                if user_question.strip():
                    with st.spinner("ğŸ§  AIê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                        response = generate_response(model, tokenizer, user_question)
                    
                    st.markdown("### ğŸ¤– AI ë¶„ì„ ê²°ê³¼")
                    st.markdown(f"**ì§ˆë¬¸:** {user_question}")
                    st.markdown(f"**ë‹µë³€:**\n{response}")
                else:
                    st.warning("âš ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        with col2:
            max_tokens = st.slider("ìµœëŒ€ í† í° ìˆ˜", 100, 1000, 512)
    
    with tab2:
        st.header("ğŸ“ˆ ì°¨íŠ¸ ë¶„ì„")
        
        # ìƒ˜í”Œ ì°¨íŠ¸ ë°ì´í„°
        chart_data = pd.DataFrame(
            np.random.randn(20, 3).cumsum(axis=0),
            columns=['KOSPI', 'KOSDAQ', 'KRX100']
        )
        
        st.line_chart(chart_data)
        
        # ê°œë³„ ì¢…ëª© ì°¨íŠ¸
        st.subheader("ğŸ“Š ê°œë³„ ì¢…ëª© ë¶„ì„")
        selected_stock = st.selectbox("ì¢…ëª© ì„ íƒ", df["íšŒì‚¬ëª…"].tolist())
        
        # ì„ íƒëœ ì¢…ëª©ì˜ ê°€ê²© ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
        dates = pd.date_range(start='2025-05-01', end='2025-06-08', freq='D')
        prices = np.random.randint(50000, 100000, len(dates))
        stock_data = pd.DataFrame({
            'ë‚ ì§œ': dates,
            'ì£¼ê°€': prices
        })
        stock_data.set_index('ë‚ ì§œ', inplace=True)
        
        st.line_chart(stock_data)
    
    with tab3:
        st.header("ğŸ“‹ ë³´ê³ ì„œ ìƒì„±")
        
        selected_company = st.selectbox(
            "ë¶„ì„í•  íšŒì‚¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
            df["íšŒì‚¬ëª…"].tolist()
        )
        
        report_type = st.radio(
            "ë³´ê³ ì„œ ìœ í˜•:",
            ["íˆ¬ì ë¶„ì„", "ì¬ë¬´ ë¶„ì„", "ê¸°ìˆ ì  ë¶„ì„", "ì¢…í•© ë¶„ì„"]
        )
        
        if st.button("ğŸ“„ ë³´ê³ ì„œ ìƒì„±", type="primary"):
            if report_type == "íˆ¬ì ë¶„ì„":
                report_prompt = f"{selected_company}ì˜ ìƒì„¸í•œ íˆ¬ì ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì¬ë¬´ìƒíƒœ, ì„±ì¥ì„±, ë¦¬ìŠ¤í¬ ìš”ì¸ì„ í¬í•¨í•´ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”."
            elif report_type == "ì¬ë¬´ ë¶„ì„":
                report_prompt = f"{selected_company}ì˜ ì¬ë¬´ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ë§¤ì¶œ, ìˆ˜ìµì„±, ì•ˆì •ì„± ì§€í‘œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”."
            elif report_type == "ê¸°ìˆ ì  ë¶„ì„":
                report_prompt = f"{selected_company}ì˜ ê¸°ìˆ ì  ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì°¨íŠ¸ íŒ¨í„´, ê±°ë˜ëŸ‰, ê¸°ìˆ ì  ì§€í‘œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”."
            else:
                report_prompt = f"{selected_company}ì˜ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. íˆ¬ì, ì¬ë¬´, ê¸°ìˆ ì  ë¶„ì„ì„ ëª¨ë‘ í¬í•¨í•´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”."
            
            with st.spinner("ğŸ“ ë³´ê³ ì„œ ìƒì„± ì¤‘..."):
                report = generate_response(model, tokenizer, report_prompt, max_new_tokens=800)
            
            st.markdown(f"### ğŸ“‹ {selected_company} {report_type} ë³´ê³ ì„œ")
            st.markdown(report)
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            st.download_button(
                label="ğŸ“¥ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ",
                data=f"{selected_company} {report_type} ë³´ê³ ì„œ\n\n{report}",
                file_name=f"{selected_company}_{report_type}_report.txt",
                mime="text/plain"
            )
    
    with tab4:
        st.header("ğŸ“Š ë°ì´í„° ë¶„ì„")
        
        # ì£¼ì‹ ë°ì´í„° ë¶„ì„
        st.subheader("ğŸ“ˆ ì£¼ì‹ ë°ì´í„° í˜„í™©")
        st.dataframe(df, use_container_width=True)
        
        # í†µê³„ ì •ë³´
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“Š ì‹œì¥ í†µê³„")
            st.metric("ìƒìŠ¹ ì¢…ëª©", len(df[df['ë“±ë½ë¥ '] > 0]))
            st.metric("í•˜ë½ ì¢…ëª©", len(df[df['ë“±ë½ë¥ '] < 0]))
            st.metric("í‰ê·  ë“±ë½ë¥ ", f"{df['ë“±ë½ë¥ '].mean():.2f}%")
        
        with col2:
            st.subheader("ğŸ’° ì¬ë¬´ í˜„í™©")
            if not financial_df.empty:
                st.dataframe(financial_df, use_container_width=True)
        
        # ì‹œì¥ ë‰´ìŠ¤ ë¶„ì„
        st.subheader("ğŸ“° ë‰´ìŠ¤ ë¶„ì„")
        st.dataframe(news_df, use_container_width=True)
        
        # ë°ì´í„° ì‹œê°í™”
        st.subheader("ğŸ“Š ë°ì´í„° ì‹œê°í™”")
        
        # ë“±ë½ë¥  ë¶„í¬
        fig_data = df['ë“±ë½ë¥ '].values
        st.bar_chart(pd.DataFrame({'ë“±ë½ë¥ ': fig_data}, index=df['íšŒì‚¬ëª…']))
        
        # ê±°ë˜ëŸ‰ ë¶„ì„
        st.subheader("ğŸ“Š ê±°ë˜ëŸ‰ ë¶„ì„")
        volume_data = df[['íšŒì‚¬ëª…', 'ê±°ë˜ëŸ‰']].set_index('íšŒì‚¬ëª…')
        st.bar_chart(volume_data)
    
    # í‘¸í„°
    st.markdown("---")
    st.markdown("**ğŸ¤– Powered by KoAlpaca-Polyglot-5.8B | CPU ìµœì í™” ë²„ì „**")
    st.markdown("**ğŸ“Š ì‹¤ì‹œê°„ KOSPI ë°ì´í„° ë¶„ì„ | ğŸ“‹ AI ë³´ê³ ì„œ ìƒì„±**")

if __name__ == "__main__":
    main()
