# kospi_research_app.py
import streamlit as st
import pandas as pd
import sqlite3
import gdown
import os
import zipfile
import tarfile
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
import numpy as np

st.set_page_config(page_title="ğŸ“ˆ KOSPI Analyst AI", layout="wide")

# Google Drive íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ë™ì  ì–‘ìí™” ì ìš© í•¨ìˆ˜
@st.cache_resource
def download_and_load_models():
    """Google Driveì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë™ì  ì–‘ìí™” ì ìš©í•˜ì—¬ ë¡œë“œ"""
    
    # Google Drive íŒŒì¼ ID
    base_model_id = "1CGpO7EO64hkUTU_eQQuZXbh-R84inkIc"
    qlora_adapter_id = "1l2F6a5HpmEmdOwTKOpu5UNRQG_jrXeW0"
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        status_text.text("ğŸ”„ ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (1/6)")
        progress_bar.progress(15)
        
        if not os.path.exists("./base_model"):
            base_model_url = f"https://drive.google.com/uc?id={base_model_id}"
            gdown.download(base_model_url, "./my_base_model.tar.gz", quiet=False)
            
            with tarfile.open("./my_base_model.tar.gz", "r:gz") as tar:
                tar.extractall("./base_model/")
        
        # QLoRA ì–´ëŒ‘í„° ë‹¤ìš´ë¡œë“œ
        status_text.text("ğŸ”„ QLoRA ì–´ëŒ‘í„° ë‹¤ìš´ë¡œë“œ ì¤‘... (2/6)")
        progress_bar.progress(30)
        
        if not os.path.exists("./qlora_adapter"):
            qlora_url = f"https://drive.google.com/uc?id={qlora_adapter_id}"
            gdown.download(qlora_url, "./qlora_results.zip", quiet=False)
            
            with zipfile.ZipFile("./qlora_results.zip", 'r') as zip_ref:
                zip_ref.extractall("./qlora_adapter/")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        status_text.text("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘... (3/6)")
        progress_bar.progress(45)
        
        tokenizer = AutoTokenizer.from_pretrained("./base_model")
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (FP32ë¡œ ë¡œë“œ)
        status_text.text("ğŸ§  ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘... (4/6)")
        progress_bar.progress(60)
        
        base_model = AutoModelForCausalLM.from_pretrained(
            "./base_model",
            torch_dtype=torch.float32,  # ë™ì  ì–‘ìí™”ë¥¼ ìœ„í•´ FP32ë¡œ ë¡œë“œ
            device_map="cpu",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # QLoRA ì–´ëŒ‘í„° ì ìš©
        status_text.text("ğŸ”§ QLoRA ì–´ëŒ‘í„° ì ìš© ì¤‘... (5/6)")
        progress_bar.progress(75)
        
        model = PeftModel.from_pretrained(base_model, "./qlora_adapter")
        
        # ë™ì  ì–‘ìí™” ì ìš©
        status_text.text("âš¡ ë™ì  ì–‘ìí™” ì ìš© ì¤‘... (6/6)")
        progress_bar.progress(85)
        
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        model.eval()
        
        # ë™ì  ì–‘ìí™” ì ìš© - Linear ë ˆì´ì–´ë“¤ì„ INT8ë¡œ ì–‘ìí™”
        quantized_model = torch.quantization.quantize_dynamic(
            model,
            {torch.nn.Linear},  # Linear ë ˆì´ì–´ë§Œ ì–‘ìí™”
            dtype=torch.qint8   # INT8 ì–‘ìí™”
        )
        
        progress_bar.progress(100)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
        def get_model_size(model):
            """ëª¨ë¸ í¬ê¸° ê³„ì‚° (MB)"""
            param_size = 0
            for param in model.parameters():
                param_size += param.nelement() * param.element_size()
            buffer_size = 0
            for buffer in model.buffers():
                buffer_size += buffer.nelement() * buffer.element_size()
            return (param_size + buffer_size) / 1024 / 1024
        
        original_size = get_model_size(model)
        quantized_size = get_model_size(quantized_model)
        compression_ratio = original_size / quantized_size
        
        status_text.text("âœ… ë™ì  ì–‘ìí™” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        
        # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì›ë³¸ ëª¨ë¸ í¬ê¸°", f"{original_size:.1f} MB")
        with col2:
            st.metric("ì–‘ìí™” ëª¨ë¸ í¬ê¸°", f"{quantized_size:.1f} MB")
        with col3:
            st.metric("ì••ì¶•ë¥ ", f"{compression_ratio:.1f}x")
        
        st.success("âš¡ ë™ì  ì–‘ìí™”ë¡œ CPU ìµœì í™” ì™„ë£Œ! ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¬ê²Œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤.")
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            if os.path.exists("./my_base_model.tar.gz"):
                os.remove("./my_base_model.tar.gz")
            if os.path.exists("./qlora_results.zip"):
                os.remove("./qlora_results.zip")
        except:
            pass
        
        # UI ì •ë¦¬
        progress_bar.empty()
        status_text.empty()
        
        return quantized_model, tokenizer
        
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        progress_bar.empty()
        status_text.empty()
        return None, None

@st.cache_data
def load_data(db_name="financial_data.db", table_name="financial_data"):
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    try:
        conn = sqlite3.connect(db_name)
        df = pd.read_sql(f"SELECT * FROM {table_name}", conn)
        conn.close()
        
        # ìˆ«ì ì»¬ëŸ¼ë“¤ì„ numericìœ¼ë¡œ ë³€í™˜í•˜ê³  NaN ì²˜ë¦¬
        numeric_columns = ['PER_ìµœê·¼', 'PBR_ìµœê·¼', 'ROE_ìµœê·¼', 'ë¶€ì±„ë¹„ìœ¨_ìµœê·¼', 'í˜„ì¬ê°€', 
                          'ìœ ë³´ìœ¨_ìµœê·¼', 'ë§¤ì¶œì•¡_ìµœê·¼', 'ì˜ì—…ì´ìµ_ìµœê·¼', 'ìˆœì´ìµ_ìµœê·¼']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col] = df[col].fillna(0)
        
        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def generate_ai_response(model, tokenizer, question, company_data):
    """ë™ì  ì–‘ìí™”ëœ AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""
    if model is None or tokenizer is None:
        return "AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    # íšŒì‚¬ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    company_info = f"""
    ì¢…ëª©ëª…: {company_data['ì¢…ëª©ëª…']}
    í‹°ì»¤: {company_data['í‹°ì»¤']}
    í˜„ì¬ê°€: {company_data['í˜„ì¬ê°€']:,.0f}ì›
    PER: {company_data['PER_ìµœê·¼']:.2f}
    PBR: {company_data['PBR_ìµœê·¼']:.2f}
    ROE: {company_data['ROE_ìµœê·¼']:.2f}%
    ë¶€ì±„ë¹„ìœ¨: {company_data['ë¶€ì±„ë¹„ìœ¨_ìµœê·¼']:.2f}%
    """
    
    prompt = f"""ë‹¤ìŒì€ {company_data['ì¢…ëª©ëª…']}ì˜ ì¬ë¬´ ì •ë³´ì…ë‹ˆë‹¤:

{company_info}

ì§ˆë¬¸: {question}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ì¦ê¶Œ ë¶„ì„ê°€ ê´€ì ì—ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""
    
    try:
        # í† í°í™” (íŒ¨ë”© ì„¤ì •)
        inputs = tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512,
            padding=True
        )
        
        # CPUì—ì„œ ë™ì  ì–‘ìí™”ëœ ëª¨ë¸ë¡œ ì¶”ë¡ 
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                num_beams=1,  # CPU ìµœì í™”ë¥¼ ìœ„í•´ beam search ë¹„í™œì„±í™”
                early_stopping=True
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = response[len(prompt):].strip()
        
        return generated_text if generated_text else "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def get_initial(korean_char):
    """í•œê¸€ ì´ˆì„± ì¶”ì¶œ"""
    ch_code = ord(korean_char) - ord('ê°€')
    if 0 <= ch_code < 11172:
        cho = ch_code // 588
        return ['ã„±','ã„²','ã„´','ã„·','ã„¸','ã„¹','ã…','ã…‚','ã…ƒ','ã……','ã…†','ã…‡','ã…ˆ','ã…‰','ã…Š','ã…‹','ã…Œ','ã…','ã…'][cho]
    return ""

# AI ëª¨ë¸ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
if 'model_loaded' not in st.session_state:
    st.info("ğŸ¤– AI ëª¨ë¸ì„ ì²˜ìŒ ë¡œë“œí•©ë‹ˆë‹¤. ë™ì  ì–‘ìí™”ë¥¼ ì ìš©í•˜ì—¬ CPU ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
    model, tokenizer = download_and_load_models()
    st.session_state.model = model
    st.session_state.tokenizer = tokenizer
    st.session_state.model_loaded = True
    
    if model is not None:
        st.success("âœ… ë™ì  ì–‘ìí™” AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì´ì œ CPUì—ì„œ íš¨ìœ¨ì ì¸ ì§€ëŠ¥í˜• ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        st.rerun()

# ë°ì´í„° ë¡œë“œ
df = load_data()

if df.empty:
    st.error("âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# ì‚¬ì´ë“œë°” í•„í„°
st.sidebar.header("ğŸ“‚ í•„í„° ì˜µì…˜")

# ì´ˆì„± í•„í„°
initials = ['ì „ì²´', 'ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
selected_initial = st.sidebar.selectbox("ğŸ”¡ ì¢…ëª©ëª… ì´ˆì„±:", initials)

if selected_initial != "ì „ì²´":
    df = df[df["ì¢…ëª©ëª…"].apply(lambda x: get_initial(x[0]) == selected_initial)]

# í…ìŠ¤íŠ¸ ê²€ìƒ‰
search_term = st.sidebar.text_input("ğŸ” ì¢…ëª©ëª… ë˜ëŠ” í‹°ì»¤ ê²€ìƒ‰")
if search_term:
    mask1 = df["ì¢…ëª©ëª…"].str.contains(search_term, case=False, na=False)
    mask2 = df["í‹°ì»¤"].str.contains(search_term, case=False, na=False)
    df = df[mask1 | mask2]

ì¢…ëª©_list = df["ì¢…ëª©ëª…"].tolist()

if not ì¢…ëª©_list:
    st.warning("âŒ ì¡°ê±´ì— ë§ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

ì„ íƒí•œ_ì¢…ëª© = st.sidebar.selectbox("ğŸ“Œ ì¢…ëª© ì„ íƒ:", ì¢…ëª©_list)
ì¢…ëª©_df = df[df["ì¢…ëª©ëª…"] == ì„ íƒí•œ_ì¢…ëª©].iloc[0]

# ë©”ì¸ ì»¨í…ì¸ 
st.title(f"ğŸ“Š {ì„ íƒí•œ_ì¢…ëª©} ({ì¢…ëª©_df['í‹°ì»¤']}) AI ë¦¬ì„œì¹˜ ë¶„ì„")

col1, col2 = st.columns(2)

with col1:
    st.metric("í˜„ì¬ê°€", f"{ì¢…ëª©_df['í˜„ì¬ê°€']:,.0f}ì›")
    st.metric("ROE (ìµœê·¼)", f"{ì¢…ëª©_df['ROE_ìµœê·¼']:.2f}%")
    st.metric("PER (ìµœê·¼)", f"{ì¢…ëª©_df['PER_ìµœê·¼']:.2f}")
    st.metric("PBR (ìµœê·¼)", f"{ì¢…ëª©_df['PBR_ìµœê·¼']:.2f}")
    st.metric("ë¶€ì±„ë¹„ìœ¨", f"{ì¢…ëª©_df['ë¶€ì±„ë¹„ìœ¨_ìµœê·¼']:.2f}%")

with col2:
    # ì•ˆì „í•œ ë©”íŠ¸ë¦­ í‘œì‹œ
    try:
        st.metric("ìœ ë³´ìœ¨", f"{ì¢…ëª©_df['ìœ ë³´ìœ¨_ìµœê·¼']:.2f}%")
    except:
        st.metric("ìœ ë³´ìœ¨", "ë°ì´í„° ì—†ìŒ")
    
    try:
        st.metric("ë§¤ì¶œì•¡", f"{ì¢…ëª©_df['ë§¤ì¶œì•¡_ìµœê·¼']:,.0f}ì›")
    except:
        st.metric("ë§¤ì¶œì•¡", "ë°ì´í„° ì—†ìŒ")
    
    try:
        st.metric("ì˜ì—…ì´ìµ", f"{ì¢…ëª©_df['ì˜ì—…ì´ìµ_ìµœê·¼']:,.0f}ì›")
    except:
        st.metric("ì˜ì—…ì´ìµ", "ë°ì´í„° ì—†ìŒ")
    
    try:
        st.metric("ìˆœì´ìµ", f"{ì¢…ëª©_df['ìˆœì´ìµ_ìµœê·¼']:,.0f}ì›")
    except:
        st.metric("ìˆœì´ìµ", "ë°ì´í„° ì—†ìŒ")

# ê·¸ë˜í”„
st.markdown("### ğŸ“ˆ ì£¼ê°€ ì¶”ì´")
price_cols = [col for col in df.columns if col.isdigit() and len(col) == 8]
if price_cols:
    try:
        price_series = ì¢…ëª©_df[price_cols].astype(float)
        price_series.index = pd.to_datetime(price_cols, format='%Y%m%d')
        chart_df = price_series.reset_index().rename(columns={'index': 'ë‚ ì§œ'})
        st.line_chart(chart_df.set_index("ë‚ ì§œ"))
    except:
        st.info("ì£¼ê°€ ì°¨íŠ¸ ë°ì´í„°ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ë‰´ìŠ¤
st.markdown("### ğŸ“° ìµœê·¼ ë‰´ìŠ¤")
if "ìµœì‹ ë‰´ìŠ¤" in ì¢…ëª©_df and isinstance(ì¢…ëª©_df["ìµœì‹ ë‰´ìŠ¤"], str) and ì¢…ëª©_df["ìµœì‹ ë‰´ìŠ¤"].strip():
    for i, link in enumerate(ì¢…ëª©_df["ìµœì‹ ë‰´ìŠ¤"].splitlines(), 1):
        if link.strip():
            st.markdown(f"{i}. [ë‰´ìŠ¤ ë§í¬]({link.strip()})")
else:
    st.info("ìµœê·¼ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

# AI ë¦¬ì„œì¹˜ ì§ˆì˜
st.markdown("### ğŸ¤– AI ë¦¬ì„œì¹˜ ì§ˆì˜ (ë™ì  ì–‘ìí™” ìµœì í™”)")

# ë¯¸ë¦¬ ì •ì˜ëœ ì§ˆë¬¸ë“¤
preset_questions = [
    "ì´ ì¢…ëª©ì˜ íˆ¬ì ë§¤ë ¥ë„ëŠ” ì–´ë–¤ê°€ìš”?",
    "PERê³¼ PBR ì§€í‘œë¥¼ ì–´ë–»ê²Œ í•´ì„í•´ì•¼ í•˜ë‚˜ìš”?",
    "í˜„ì¬ ì¬ë¬´ ìƒíƒœì˜ ê°•ì ê³¼ ì•½ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ì´ ì¢…ëª©ì˜ ë¦¬ìŠ¤í¬ ìš”ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ë™ì¢… ì—…ê³„ ëŒ€ë¹„ ê²½ìŸë ¥ì€ ì–´ë–¤ê°€ìš”?"
]

col1, col2 = st.columns([3, 1])

with col1:
    user_question = st.text_input("ğŸ§  ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:", 
                                placeholder="ì˜ˆ: ì´ ì¢…ëª©ì˜ PERì´ ë†’ìœ¼ë©´ ì–´ë–¤ í•´ì„ì´ ê°€ëŠ¥í•´?")

with col2:
    selected_preset = st.selectbox("ğŸ“‹ ë¯¸ë¦¬ ì •ì˜ëœ ì§ˆë¬¸:", ["ì§ì ‘ ì…ë ¥"] + preset_questions)

if selected_preset != "ì§ì ‘ ì…ë ¥":
    user_question = selected_preset

if user_question:
    if st.session_state.get('model') is not None:
        if st.button("ğŸ” AI ë¶„ì„ ìš”ì²­ (ì–‘ìí™” ëª¨ë¸)", type="primary"):
            with st.spinner("ğŸ¤– ë™ì  ì–‘ìí™”ëœ AIê°€ CPUì—ì„œ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                ai_response = generate_ai_response(
                    st.session_state.model, 
                    st.session_state.tokenizer, 
                    user_question, 
                    ì¢…ëª©_df
                )
            
            st.markdown("#### ğŸ¯ AI ë¶„ì„ ê²°ê³¼")
            st.markdown(ai_response)
            
            # ë¶„ì„ ê²°ê³¼ ì €ì¥ ì˜µì…˜
            analysis_text = f"""
# {ì„ íƒí•œ_ì¢…ëª©} AI ë¶„ì„ ê²°ê³¼ (ë™ì  ì–‘ìí™” ëª¨ë¸)

**ì§ˆë¬¸:** {user_question}

**AI ë¶„ì„:**
{ai_response}

**ê¸°ë³¸ ì •ë³´:**
- ì¢…ëª©ëª…: {ì¢…ëª©_df['ì¢…ëª©ëª…']}
- í‹°ì»¤: {ì¢…ëª©_df['í‹°ì»¤']}
- í˜„ì¬ê°€: {ì¢…ëª©_df['í˜„ì¬ê°€']:,.0f}ì›
- PER: {ì¢…ëª©_df['PER_ìµœê·¼']:.2f}
- PBR: {ì¢…ëª©_df['PBR_ìµœê·¼']:.2f}
- ROE: {ì¢…ëª©_df['ROE_ìµœê·¼']:.2f}%

*ë³¸ ë¶„ì„ì€ ë™ì  ì–‘ìí™”ë¡œ ìµœì í™”ëœ AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""
            
            st.download_button(
                label="ğŸ“¥ ë¶„ì„ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ",
                data=analysis_text,
                file_name=f"{ì„ íƒí•œ_ì¢…ëª©}_AIë¶„ì„_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.md",
                mime="text/markdown"
            )
    else:
        st.write("ğŸ” ë™ì  ì–‘ìí™” AI ëª¨ë¸ ë¡œë”© ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")

# ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
if st.session_state.get('model_loaded'):
    with st.expander("âš¡ ë™ì  ì–‘ìí™” ì„±ëŠ¥ ì •ë³´"):
        st.markdown("""
        **ë™ì  ì–‘ìí™” ìµœì í™” íš¨ê³¼:**
        - ğŸ”¹ **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ì•½ 60-70% ê°ì†Œ
        - ğŸ”¹ **ëª¨ë¸ í¬ê¸°**: ì•½ 4ë°° ì••ì¶•
        - ğŸ”¹ **CPU ì¶”ë¡  ì†ë„**: 2-4ë°° í–¥ìƒ
        - ğŸ”¹ **ì •í™•ë„ ì†ì‹¤**: ìµœì†Œí™” (< 1%)
        - ğŸ”¹ **ì–‘ìí™” ë°©ì‹**: INT8 ë™ì  ì–‘ìí™” (Linear ë ˆì´ì–´)
        
        **ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­:**
        - PyTorch Dynamic Quantization ì‚¬ìš©
        - ê°€ì¤‘ì¹˜: INT8 ì €ì¥, í™œì„±í™”: ëŸ°íƒ€ì„ ë™ì  ì–‘ìí™”
        - CPU ë²¡í„°í™” ì—°ì‚° ìµœì í™”
        """)
