import os
import sys

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (í† í¬ë‚˜ì´ì € ê²½ê³  ë°©ì§€)
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# PyTorch í´ë˜ìŠ¤ ê²½ë¡œ ì¶©ëŒ í•´ê²° (ìµœìƒë‹¨ì— ìœ„ì¹˜)
try:
    import torch
    import importlib.util
    torch_classes_path = os.path.join(os.path.dirname(importlib.util.find_spec("torch").origin), "classes")
    if hasattr(torch, "classes"):
        torch.classes.__path__ = [torch_classes_path]
except Exception as e:
    pass  # torch ë¯¸ì„¤ì¹˜ ì‹œ ë¬´ì‹œ

import streamlit as st

# transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ import ë° ìƒíƒœ ì²´í¬
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    st.error(f"Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

st.title("ğŸ¤– ì‘ì€ LLM ëª¨ë¸ ë¡œë“œ í™•ì¸")

if not TRANSFORMERS_AVAILABLE:
    st.stop()

@st.cache_resource
def load_simple_model():
    """ê°€ì¥ ì‘ì€ GPT ëª¨ë¸ ë¡œë“œ"""
    try:
        model_name = "sshleifer/tiny-gpt2"  # ë§¤ìš° ì‘ì€ í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)

        # íŒ¨ë”© í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        return model, tokenizer, "âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!"
    except Exception as e:
        return None, None, f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"

# ëª¨ë¸ ë¡œë“œ
with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘..."):
    model, tokenizer, status = load_simple_model()

st.write(status)

if model is not None and tokenizer is not None:
    st.success("ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")

    # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
    st.subheader("í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸")

    prompt = st.text_input("í”„ë¡¬í”„íŠ¸ ì…ë ¥:", "Hello")

    if st.button("ìƒì„±"):
        if prompt:
            with st.spinner("ìƒì„± ì¤‘..."):
                try:
                    # í† í°í™”
                    inputs = tokenizer.encode(prompt, return_tensors="pt")

                    # ìƒì„±
                    with torch.no_grad():
                        outputs = model.generate(
                            inputs,
                            max_length=inputs.shape[1] + 20,
                            num_return_sequences=1,
                            temperature=0.7,
                            do_sample=True,
                            pad_token_id=tokenizer.eos_token_id
                        )

                    # ë””ì½”ë”©
                    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

                    st.write("**ìƒì„± ê²°ê³¼:**")
                    st.write(result)

                except Exception as e:
                    st.error(f"ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        else:
            st.warning("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
else:
    st.error("ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ì‹œìŠ¤í…œ ì •ë³´
st.sidebar.write("**ì‹œìŠ¤í…œ ì •ë³´:**")
st.sidebar.write(f"Python: {sys.version.split()[0]}")

if TRANSFORMERS_AVAILABLE:
    st.sidebar.write(f"PyTorch: {torch.__version__}")
    st.sidebar.write(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        st.sidebar.write(f"CUDA ì¥ì¹˜: {torch.cuda.get_device_name(0)}")
else:
    st.sidebar.write("PyTorch: ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨")
