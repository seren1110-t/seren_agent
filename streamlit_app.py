import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import zipfile
import os
import tempfile
import gdown
import time
from pathlib import Path
import json

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="KoAlpaca CPU ëª¨ë¸ í…ŒìŠ¤íŠ¸",
    page_icon="ğŸ¤–",
    layout="wide"
)

def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if 'gdrive_file_id' not in st.session_state:
        st.session_state.gdrive_file_id = ""
    
    if 'model_loaded' not in st.session_state:
        st.session_state.model_loaded = False
    
    if 'model' not in st.session_state:
        st.session_state.model = None
    
    if 'tokenizer' not in st.session_state:
        st.session_state.tokenizer = None
    
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    if 'model_path' not in st.session_state:
        st.session_state.model_path = None

def download_and_extract_model(file_id):
    """êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ"""
    try:
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        temp_dir = tempfile.mkdtemp()
        zip_path = os.path.join(temp_dir, "koalpaca_cpu_deployment.zip")
        extract_path = os.path.join(temp_dir, "koalpaca_cpu_deployment")
        
        # êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ë‹¤ìš´ë¡œë“œ
        st.info("ğŸ”„ êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        download_url = f"https://drive.google.com/uc?id={file_id}"
        gdown.download(download_url, zip_path, quiet=False)
        
        # ì••ì¶• í•´ì œ
        st.info("ğŸ“¦ ëª¨ë¸ ì••ì¶• í•´ì œ ì¤‘...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        
        # ì••ì¶• í•´ì œëœ í´ë” êµ¬ì¡° í™•ì¸
        model_path = None
        for root, dirs, files in os.walk(extract_path):
            if "tokenizer.json" in files or "config.json" in files or "cpu_quantized_model.pt" in files:
                model_path = root
                break
        
        if model_path is None:
            # ì²« ë²ˆì§¸ í•˜ìœ„ ë””ë ‰í† ë¦¬ë¥¼ ëª¨ë¸ ê²½ë¡œë¡œ ì‚¬ìš©
            subdirs = [d for d in os.listdir(extract_path) if os.path.isdir(os.path.join(extract_path, d))]
            if subdirs:
                model_path = os.path.join(extract_path, subdirs[0])
            else:
                model_path = extract_path
        
        st.success(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_path}")
        return model_path
        
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

def check_model_files(model_path):
    """ëª¨ë¸ íŒŒì¼ êµ¬ì¡° í™•ì¸"""
    if not os.path.exists(model_path):
        return False, "ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    
    files = os.listdir(model_path)
    
    expected_files = {
        'cpu_quantized_model.pt': os.path.exists(os.path.join(model_path, 'cpu_quantized_model.pt')),
        'cpu_model_config.json': os.path.exists(os.path.join(model_path, 'cpu_model_config.json')),
        'cpu_loading_example.py': os.path.exists(os.path.join(model_path, 'cpu_loading_example.py')),
        'tokenizer_files': any(f.startswith('tokenizer') for f in files) or 'tokenizer.json' in files
    }
    
    return expected_files, f"ë°œê²¬ëœ íŒŒì¼: {files}"

def load_koalpaca_model(model_path):
    """KoAlpaca CPU ëª¨ë¸ ë¡œë“œ (ë‚´ì¥ íŒŒì¼ êµ¬ì¡°ì— ë§ì¶¤)"""
    try:
        st.info("ğŸ§  KoAlpaca ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # íŒŒì¼ êµ¬ì¡° í™•ì¸
        file_check, file_info = check_model_files(model_path)
        st.info(f"ğŸ“ {file_info}")
        
        # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            use_fast=False
        )
        
        # 2. cpu_quantized_model.pt íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        quantized_model_path = os.path.join(model_path, "cpu_quantized_model.pt")
        config_path = os.path.join(model_path, "cpu_model_config.json")
        
        if os.path.exists(quantized_model_path):
            # ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
            st.info("ğŸ“¦ ì–‘ìí™”ëœ CPU ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            # ì„¤ì • íŒŒì¼ ë¡œë“œ
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config_info = json.load(f)
                st.info(f"ğŸ“‹ ëª¨ë¸ ì„¤ì •: {config_info.get('model_type', 'Unknown')}")
            
            # ë² ì´ìŠ¤ ëª¨ë¸ ë¨¼ì € ë¡œë“œ
            model = AutoModelForCausalLM.from_pretrained(
                "beomi/KoAlpaca-Polyglot-5.8B",
                torch_dtype=torch.float32,
                device_map="cpu",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            # ì–‘ìí™”ëœ state_dict ë¡œë“œ
            checkpoint = torch.load(quantized_model_path, map_location="cpu")
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
                st.success("âœ… ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            else:
                st.warning("âš ï¸ í‘œì¤€ ëª¨ë¸ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.")
        else:
            # í‘œì¤€ ëª¨ë¸ ë¡œë“œ
            st.info("ğŸ“¦ í‘œì¤€ ëª¨ë¸ ë¡œë“œ ì¤‘...")
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                device_map="cpu",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
        
        model.eval()
        
        st.success("âœ… KoAlpaca ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        return model, tokenizer
        
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None, None

def generate_response(model, tokenizer, question, max_new_tokens=200):
    """ì‘ë‹µ ìƒì„±"""
    try:
        # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        prompt = f"### ì§ˆë¬¸: {question}\n\n### ë‹µë³€:"
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            prompt, 
            return_tensors="pt",
            max_length=512,
            truncation=True,
            padding=True
        )
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1,
                no_repeat_ngram_size=3
            )
        
        # ì‘ë‹µ ë””ì½”ë”©
        generated_ids = outputs[0][len(inputs.input_ids[0]):]
        response = tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        return response.strip()
        
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def render_sidebar():
    """ì‚¬ì´ë“œë°” ë Œë”ë§"""
    with st.sidebar:
        st.header("ğŸ“‹ ëª¨ë¸ ì •ë³´")
        st.info("""
        **ëª¨ë¸**: KoAlpaca-Polyglot-5.8B CPU ë²„ì „
        **ìš©ë„**: í•œêµ­ì–´ ì§ˆì˜ì‘ë‹µ
        **ìµœì í™”**: CPU ì¶”ë¡ ìš©
        """)
        
        st.header("ğŸ”§ ì„¤ì •")
        max_tokens = st.slider("ìµœëŒ€ í† í° ìˆ˜", 50, 500, 200)
        
        st.header("ğŸ“ ì‚¬ìš©ë²•")
        st.markdown("""
        1. êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”
        2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
        3. ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”
        """)
        
        # ëª¨ë¸ ìƒíƒœ í‘œì‹œ
        st.header("ğŸ” ëª¨ë¸ ìƒíƒœ")
        if st.session_state.model_loaded:
            st.success("âœ… ëª¨ë¸ ë¡œë“œë¨")
        else:
            st.warning("âš ï¸ ëª¨ë¸ ë¡œë“œ í•„ìš”")
        
        return max_tokens

def render_file_id_input():
    """íŒŒì¼ ID ì…ë ¥ ì„¹ì…˜ ë Œë”ë§"""
    st.header("ğŸ”— êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì„¤ì •")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        file_id = st.text_input(
            "êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ ID ì…ë ¥",
            value=st.session_state.gdrive_file_id,
            help="êµ¬ê¸€ ë“œë¼ì´ë¸Œ ê³µìœ  ë§í¬ì—ì„œ íŒŒì¼ IDë¥¼ ì¶”ì¶œí•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”",
            key="file_id_input"
        )
    
    with col2:
        st.markdown("**íŒŒì¼ ID ì¶”ì¶œ ë°©ë²•:**")
        st.code("https://drive.google.com/file/d/FILE_ID/view")
        st.caption("FILE_ID ë¶€ë¶„ì„ ë³µì‚¬í•˜ì„¸ìš”")
    
    # session_state ì—…ë°ì´íŠ¸
    if file_id != st.session_state.gdrive_file_id:
        st.session_state.gdrive_file_id = file_id
    
    return file_id

def render_model_download():
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ ë Œë”ë§"""
    st.header("ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ")
    
    if not st.session_state.gdrive_file_id:
        st.warning("âš ï¸ êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return False
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        download_btn = st.button("ğŸ”„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘", type="primary")
    
    with col2:
        if st.button("ğŸ—‘ï¸ ëª¨ë¸ ì´ˆê¸°í™”"):
            st.session_state.model_loaded = False
            st.session_state.model = None
            st.session_state.tokenizer = None
            st.session_state.model_path = None
            st.rerun()
    
    if download_btn:
        with st.spinner("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
            model_path = download_and_extract_model(st.session_state.gdrive_file_id)
            
            if model_path:
                st.session_state.model_path = model_path
                model, tokenizer = load_koalpaca_model(model_path)
                
                if model and tokenizer:
                    st.session_state.model = model
                    st.session_state.tokenizer = tokenizer
                    st.session_state.model_loaded = True
                    st.success("ğŸ‰ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì´ì œ ì§ˆë¬¸ì„ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    st.rerun()
                else:
                    st.error("âŒ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.error("âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    return st.session_state.model_loaded

def render_qa_interface(max_tokens):
    """ì§ˆì˜ì‘ë‹µ ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§"""
    st.header("ğŸ’¬ ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸")
    
    # ì˜ˆì‹œ ì§ˆë¬¸ë“¤
    example_questions = [
        "ì‚¼ì„±ì „ìì˜ ì¬ë¬´ìƒíƒœëŠ” ì–´ë–»ìŠµë‹ˆê¹Œ?",
        "ë„¤ì´ë²„ì˜ ì‚¬ì—… ì „ë§ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.",
        "ì¹´ì¹´ì˜¤ì˜ ì£¼ìš” ì‚¬ì—… ì˜ì—­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í•œêµ­ IT ì‚°ì—…ì˜ í˜„í™©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "íˆ¬ìí•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ìš”ì†Œë“¤ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    # ì§ˆë¬¸ ì…ë ¥
    col1, col2 = st.columns([3, 1])
    
    with col1:
        question = st.text_area(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
            height=100,
            placeholder="ì˜ˆ: ì‚¼ì„±ì „ìì˜ ì¬ë¬´ìƒíƒœëŠ” ì–´ë–»ìŠµë‹ˆê¹Œ?",
            key="question_input"
        )
    
    with col2:
        st.markdown("**ì˜ˆì‹œ ì§ˆë¬¸:**")
        for i, ex_q in enumerate(example_questions):
            if st.button(f"ì˜ˆì‹œ {i+1}", key=f"example_{i}"):
                st.session_state.question_input = ex_q
                st.rerun()
    
    # ì‘ë‹µ ìƒì„± ë²„íŠ¼
    col1, col2, col3 = st.columns([1, 1, 2])
    
    with col1:
        generate_btn = st.button("ğŸš€ ì‘ë‹µ ìƒì„±", type="primary")
    
    with col2:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.chat_history = []
            st.rerun()
    
    # ì‘ë‹µ ìƒì„± ì²˜ë¦¬
    if generate_btn and question.strip():
        if st.session_state.model and st.session_state.tokenizer:
            with st.spinner("ğŸ¤” ì‘ë‹µ ìƒì„± ì¤‘..."):
                start_time = time.time()
                
                response = generate_response(
                    st.session_state.model, 
                    st.session_state.tokenizer, 
                    question,
                    max_tokens
                )
                
                end_time = time.time()
                generation_time = end_time - start_time
                
                # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                st.session_state.chat_history.append({
                    'question': question,
                    'response': response,
                    'time': generation_time
                })
                
                st.rerun()
        else:
            st.error("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def render_chat_history():
    """ëŒ€í™” ê¸°ë¡ ë Œë”ë§"""
    if st.session_state.chat_history:
        st.header("ğŸ“œ ëŒ€í™” ê¸°ë¡")
        
        for i, chat in enumerate(reversed(st.session_state.chat_history)):
            with st.expander(f"ğŸ’¬ ëŒ€í™” {len(st.session_state.chat_history) - i}", expanded=(i == 0)):
                st.markdown(f"**â“ ì§ˆë¬¸:** {chat['question']}")
                st.markdown(f"**ğŸ¤– ì‘ë‹µ:** {chat['response']}")
                st.caption(f"â±ï¸ ìƒì„± ì‹œê°„: {chat['time']:.2f}ì´ˆ")

def render_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ë Œë”ë§"""
    with st.expander("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´"):
        system_info = {
            "Python ë²„ì „": "3.8+",
            "PyTorch ë²„ì „": torch.__version__,
            "ë””ë°”ì´ìŠ¤": "CPU",
            "ëª¨ë¸ ìƒíƒœ": "ë¡œë“œë¨" if st.session_state.model_loaded else "ë¡œë“œ ì•ˆë¨",
            "ëª¨ë¸ ê²½ë¡œ": st.session_state.model_path if st.session_state.model_path else "ì—†ìŒ",
            "ëŒ€í™” ê¸°ë¡ ìˆ˜": len(st.session_state.chat_history)
        }
        st.json(system_info)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    initialize_session_state()
    
    # í˜ì´ì§€ ì œëª©
    st.title("ğŸ¤– KoAlpaca CPU ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    st.markdown("êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ KoAlpaca CPU ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ê²€ìƒ‰ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    
    # ì‚¬ì´ë“œë°” ë Œë”ë§
    max_tokens = render_sidebar()
    
    # íŒŒì¼ ID ì…ë ¥
    file_id = render_file_id_input()
    
    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    model_loaded = render_model_download()
    
    # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œ
    if not model_loaded:
        st.info("ğŸ‘† ë¨¼ì € 'ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    # ì§ˆì˜ì‘ë‹µ ì¸í„°í˜ì´ìŠ¤
    render_qa_interface(max_tokens)
    
    # ëŒ€í™” ê¸°ë¡
    render_chat_history()
    
    # ì‹œìŠ¤í…œ ì •ë³´
    render_system_info()

if __name__ == "__main__":
    main()
