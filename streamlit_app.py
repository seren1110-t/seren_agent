import os
import sys

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (í† í¬ë‚˜ì´ì € ê²½ê³  ë°©ì§€)
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# PyTorch í´ë˜ìŠ¤ ê²½ë¡œ ì¶©ëŒ í•´ê²° (ìµœìƒë‹¨ì— ìœ„ì¹˜)
try:
    import torch
    import importlib.util
    torch_classes_path = os.path.join(os.path.dirname(importlib.util.find_spec("torch").origin), "classes")
    if hasattr(torch, "classes"):
        torch.classes.__path__ = [torch_classes_path]
except Exception as e:
    pass  # torch ë¯¸ì„¤ì¹˜ ì‹œ ë¬´ì‹œ

import streamlit as st

# transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ import ë° ìƒíƒœ ì²´í¬
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    st.error(f"Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

st.title("ğŸ¦™ TinyLlama 1.1B (CPU ì „ìš©) ë°ëª¨")

if not TRANSFORMERS_AVAILABLE:
    st.stop()

@st.cache_resource
def load_llama_model():
    """TinyLlama 1.1B ëª¨ë¸ ë¡œë“œ (CPU Only)"""
    try:
        model_name = "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32
        )
        model.to("cpu")
        
        # í† í¬ë‚˜ì´ì € ì„¤ì •
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"
            
        return model, tokenizer, "âœ… TinyLlama ë¡œë“œ ì„±ê³µ!"
    except Exception as e:
        return None, None, f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"

# ëª¨ë¸ ë¡œë“œ
with st.spinner("TinyLlama ëª¨ë¸ ë¡œë”© ì¤‘..."):
    model, tokenizer, status = load_llama_model()

st.write(status)

if model is not None and tokenizer is not None:
    st.success("TinyLlama 1.1B ì¤€ë¹„ ì™„ë£Œ!")
    
    # í…ìŠ¤íŠ¸ ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
    st.sidebar.subheader("ìƒì„± ì„¤ì •")
    max_new_tokens = st.sidebar.slider("ìµœëŒ€ í† í° ìˆ˜", 50, 500, 200)
    temperature = st.sidebar.slider("Temperature", 0.1, 1.0, 0.7)
    
    # í…ìŠ¤íŠ¸ ìƒì„± ì¸í„°í˜ì´ìŠ¤
    st.subheader("ì±— ì¸í„°í˜ì´ìŠ¤")
    
    prompt = st.text_area("í”„ë¡¬í”„íŠ¸ ì…ë ¥:", "The future of AI is", height=100)
    
    if st.button("ìƒì„± ì‹œì‘"):
        if prompt:
            with st.spinner("í…ìŠ¤íŠ¸ ìƒì„± ì¤‘..."):
                try:
                    # í† í°í™”
                    inputs = tokenizer(
                        prompt, 
                        return_tensors="pt", 
                        padding=True, 
                        truncation=True
                    )
                    
                    # ìƒì„±
                    with torch.no_grad():
                        outputs = model.generate(
                            **inputs,
                            max_new_tokens=max_new_tokens,
                            temperature=temperature,
                            do_sample=True,
                            pad_token_id=tokenizer.eos_token_id,
                            repetition_penalty=1.1
                        )
                    
                    # ë””ì½”ë”©
                    result = tokenizer.decode(
                        outputs[0], 
                        skip_special_tokens=True
                    )
                    
                    st.write("**ìƒì„± ê²°ê³¼:**")
                    st.markdown(result)
                    
                except Exception as e:
                    st.error(f"ìƒì„± ì˜¤ë¥˜: {str(e)}")
        else:
            st.warning("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
else:
    st.error("ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")

# ì‹œìŠ¤í…œ ì •ë³´
st.sidebar.subheader("ì‹œìŠ¤í…œ ìƒíƒœ")
st.sidebar.write(f"Python: {sys.version.split()[0]}")

if TRANSFORMERS_AVAILABLE:
    st.sidebar.write(f"PyTorch: {torch.__version__}")
    st.sidebar.write(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        st.sidebar.write(f"GPU: {torch.cuda.get_device_name(0)}")
