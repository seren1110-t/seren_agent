# kospi_research_app.py
import streamlit as st
import pandas as pd
import sqlite3
import gdown
import os
import zipfile
import tarfile
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
import numpy as np

st.set_page_config(page_title="ğŸ“ˆ KOSPI Analyst AI", layout="wide")

# Google Drive íŒŒì¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
@st.cache_resource
def download_and_load_models():
    """Google Driveì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ"""
    
    # Google Drive íŒŒì¼ ID (ê³µìœ  ë§í¬ì—ì„œ ì¶”ì¶œ)
    base_model_id = "1CGpO7EO64hkUTU_eQQuZXbh-R84inkIc"  # my_base_model.tar.gzì˜ íŒŒì¼ ID
    qlora_adapter_id = "1l2F6a5HpmEmdOwTKOpu5UNRQG_jrXeW0"  # qlora_results.zipì˜ íŒŒì¼ ID
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        status_text.text("ğŸ”„ ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (1/4)")
        progress_bar.progress(25)
        
        if not os.path.exists("./base_model"):
            base_model_url = f"https://drive.google.com/uc?id={base_model_id}"
            gdown.download(base_model_url, "./my_base_model.tar.gz", quiet=False)
            
            # ì••ì¶• í•´ì œ
            with tarfile.open("./my_base_model.tar.gz", "r:gz") as tar:
                tar.extractall("./base_model/")
        
        # QLoRA ì–´ëŒ‘í„° ë‹¤ìš´ë¡œë“œ
        status_text.text("ğŸ”„ QLoRA ì–´ëŒ‘í„° ë‹¤ìš´ë¡œë“œ ì¤‘... (2/4)")
        progress_bar.progress(50)
        
        if not os.path.exists("./qlora_adapter"):
            qlora_url = f"https://drive.google.com/uc?id={qlora_adapter_id}"
            gdown.download(qlora_url, "./qlora_results.zip", quiet=False)
            
            # ì••ì¶• í•´ì œ
            with zipfile.ZipFile("./qlora_results.zip", 'r') as zip_ref:
                zip_ref.extractall("./qlora_adapter/")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        status_text.text("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘... (3/4)")
        progress_bar.progress(75)
        
        tokenizer = AutoTokenizer.from_pretrained("./base_model")
        
        # ëª¨ë¸ ë¡œë“œ ë° ì–´ëŒ‘í„° ì ìš©
        status_text.text("ğŸ§  AI ëª¨ë¸ ë¡œë“œ ì¤‘... (4/4)")
        progress_bar.progress(90)
        
        base_model = AutoModelForCausalLM.from_pretrained(
            "./base_model",
            torch_dtype=torch.float16,
            device_map="cpu",
            low_cpu_mem_usage=True
        )
        
        # QLoRA ì–´ëŒ‘í„° ì ìš©
        model = PeftModel.from_pretrained(base_model, "./qlora_adapter")
        
        progress_bar.progress(100)
        status_text.text("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        
        # UI ì •ë¦¬
        progress_bar.empty()
        status_text.empty()
        
        return model, tokenizer
        
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

@st.cache_data
def load_data(db_name="financial_data.db", table_name="financial_data"):
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    try:
        conn = sqlite3.connect(db_name)
        df = pd.read_sql(f"SELECT * FROM {table_name}", conn)
        conn.close()
        
        # ìˆ«ì ì»¬ëŸ¼ë“¤ì„ numericìœ¼ë¡œ ë³€í™˜í•˜ê³  NaN ì²˜ë¦¬
        numeric_columns = ['PER_ìµœê·¼', 'PBR_ìµœê·¼', 'ROE_ìµœê·¼', 'ë¶€ì±„ë¹„ìœ¨_ìµœê·¼', 'í˜„ì¬ê°€', 
                          'ìœ ë³´ìœ¨_ìµœê·¼', 'ë§¤ì¶œì•¡_ìµœê·¼', 'ì˜ì—…ì´ìµ_ìµœê·¼', 'ìˆœì´ìµ_ìµœê·¼']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col] = df[col].fillna(0)  # NaNì„ 0ìœ¼ë¡œ ëŒ€ì²´
        
        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def generate_ai_response(model, tokenizer, question, company_data):
    """AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""
    if model is None or tokenizer is None:
        return "AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    # íšŒì‚¬ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    company_info = f"""
    ì¢…ëª©ëª…: {company_data['ì¢…ëª©ëª…']}
    í‹°ì»¤: {company_data['í‹°ì»¤']}
    í˜„ì¬ê°€: {company_data['í˜„ì¬ê°€']}
    PER: {company_data['PER_ìµœê·¼']}
    PBR: {company_data['PBR_ìµœê·¼']}
    ROE: {company_data['ROE_ìµœê·¼']}
    ë¶€ì±„ë¹„ìœ¨: {company_data['ë¶€ì±„ë¹„ìœ¨_ìµœê·¼']}
    """
    
    prompt = f"""ë‹¤ìŒì€ {company_data['ì¢…ëª©ëª…']}ì˜ ì¬ë¬´ ì •ë³´ì…ë‹ˆë‹¤:

{company_info}

ì§ˆë¬¸: {question}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ì¦ê¶Œ ë¶„ì„ê°€ ê´€ì ì—ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""
    
    try:
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°
        generated_text = response[len(prompt):].strip()
        
        return generated_text
        
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def get_initial(korean_char):
    """í•œê¸€ ì´ˆì„± ì¶”ì¶œ"""
    ch_code = ord(korean_char) - ord('ê°€')
    if 0 <= ch_code < 11172:
        cho = ch_code // 588
        return ['ã„±','ã„²','ã„´','ã„·','ã„¸','ã„¹','ã…','ã…‚','ã…ƒ','ã……','ã…†','ã…‡','ã…ˆ','ã…‰','ã…Š','ã…‹','ã…Œ','ã…','ã…'][cho]
    return ""

# AI ëª¨ë¸ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
if 'model_loaded' not in st.session_state:
    st.info("ğŸ¤– AI ëª¨ë¸ì„ ì²˜ìŒ ë¡œë“œí•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    model, tokenizer = download_and_load_models()
    st.session_state.model = model
    st.session_state.tokenizer = tokenizer
    st.session_state.model_loaded = True
    
    if model is not None:
        st.success("âœ… AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì´ì œ ì§€ëŠ¥í˜• ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        st.rerun()

# ë°ì´í„° ë¡œë“œ
df = load_data()

if df.empty:
    st.error("âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# ì‚¬ì´ë“œë°” í•„í„° (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
st.sidebar.header("ğŸ“‚ í•„í„° ì˜µì…˜")

# ì´ˆì„± í•„í„°
initials = ['ì „ì²´', 'ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
selected_initial = st.sidebar.selectbox("ğŸ”¡ ì¢…ëª©ëª… ì´ˆì„±:", initials)

if selected_initial != "ì „ì²´":
    df = df[df["ì¢…ëª©ëª…"].apply(lambda x: get_initial(x[0]) == selected_initial)]

# í…ìŠ¤íŠ¸ ê²€ìƒ‰
search_term = st.sidebar.text_input("ğŸ” ì¢…ëª©ëª… ë˜ëŠ” í‹°ì»¤ ê²€ìƒ‰")
if search_term:
    mask1 = df["ì¢…ëª©ëª…"].str.contains(search_term, case=False, na=False)
    mask2 = df["í‹°ì»¤"].str.contains(search_term, case=False, na=False)
    df = df[mask1 | mask2]

ì¢…ëª©_list = df["ì¢…ëª©ëª…"].tolist()

if not ì¢…ëª©_list:
    st.warning("âŒ ì¡°ê±´ì— ë§ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

ì„ íƒí•œ_ì¢…ëª© = st.sidebar.selectbox("ğŸ“Œ ì¢…ëª© ì„ íƒ:", ì¢…ëª©_list)
ì¢…ëª©_df = df[df["ì¢…ëª©ëª…"] == ì„ íƒí•œ_ì¢…ëª©].iloc[0]

# ----------------------- ë©”ì¸ ì»¨í…ì¸  (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€) -----------------------
st.title(f"ğŸ“Š {ì„ íƒí•œ_ì¢…ëª©} ({ì¢…ëª©_df['í‹°ì»¤']}) AI ë¦¬ì„œì¹˜ ë¶„ì„")

col1, col2 = st.columns(2)

with col1:
    st.metric("í˜„ì¬ê°€", f"{ì¢…ëª©_df['í˜„ì¬ê°€']:,.0f}ì›")
    st.metric("ROE (ìµœê·¼)", f"{ì¢…ëª©_df['ROE_ìµœê·¼']:.2f}%")
    st.metric("PER (ìµœê·¼)", f"{ì¢…ëª©_df['PER_ìµœê·¼']:.2f}")
    st.metric("PBR (ìµœê·¼)", f"{ì¢…ëª©_df['PBR_ìµœê·¼']:.2f}")
    st.metric("ë¶€ì±„ë¹„ìœ¨", f"{ì¢…ëª©_df['ë¶€ì±„ë¹„ìœ¨_ìµœê·¼']:.2f}%")

with col2:
    # ì•ˆì „í•œ ë©”íŠ¸ë¦­ í‘œì‹œ
    try:
        st.metric("ìœ ë³´ìœ¨", f"{ì¢…ëª©_df['ìœ ë³´ìœ¨_ìµœê·¼']:.2f}%")
    except:
        st.metric("ìœ ë³´ìœ¨", "ë°ì´í„° ì—†ìŒ")
    
    try:
        st.metric("ë§¤ì¶œì•¡", f"{ì¢…ëª©_df['ë§¤ì¶œì•¡_ìµœê·¼']:,.0f}ì›")
    except:
        st.metric("ë§¤ì¶œì•¡", "ë°ì´í„° ì—†ìŒ")
    
    try:
        st.metric("ì˜ì—…ì´ìµ", f"{ì¢…ëª©_df['ì˜ì—…ì´ìµ_ìµœê·¼']:,.0f}ì›")
    except:
        st.metric("ì˜ì—…ì´ìµ", "ë°ì´í„° ì—†ìŒ")
    
    try:
        st.metric("ìˆœì´ìµ", f"{ì¢…ëª©_df['ìˆœì´ìµ_ìµœê·¼']:,.0f}ì›")
    except:
        st.metric("ìˆœì´ìµ", "ë°ì´í„° ì—†ìŒ")

# ê·¸ë˜í”„ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
st.markdown("### ğŸ“ˆ ì£¼ê°€ ì¶”ì´")
price_cols = [col for col in df.columns if col.isdigit() and len(col) == 8]
if price_cols:
    try:
        price_series = ì¢…ëª©_df[price_cols].astype(float)
        price_series.index = pd.to_datetime(price_cols, format='%Y%m%d')
        chart_df = price_series.reset_index().rename(columns={'index': 'ë‚ ì§œ'})
        st.line_chart(chart_df.set_index("ë‚ ì§œ"))
    except:
        st.info("ì£¼ê°€ ì°¨íŠ¸ ë°ì´í„°ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ë‰´ìŠ¤ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
st.markdown("### ğŸ“° ìµœê·¼ ë‰´ìŠ¤")
if "ìµœì‹ ë‰´ìŠ¤" in ì¢…ëª©_df and isinstance(ì¢…ëª©_df["ìµœì‹ ë‰´ìŠ¤"], str) and ì¢…ëª©_df["ìµœì‹ ë‰´ìŠ¤"].strip():
    for i, link in enumerate(ì¢…ëª©_df["ìµœì‹ ë‰´ìŠ¤"].splitlines(), 1):
        if link.strip():
            st.markdown(f"{i}. [ë‰´ìŠ¤ ë§í¬]({link.strip()})")
else:
    st.info("ìµœê·¼ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

# AI ë¦¬ì„œì¹˜ ì§ˆì˜ (ê¸°ì¡´ êµ¬ì¡°ë¥¼ í™•ì¥)
st.markdown("### ğŸ¤– AI ë¦¬ì„œì¹˜ ì§ˆì˜")

# ë¯¸ë¦¬ ì •ì˜ëœ ì§ˆë¬¸ë“¤
preset_questions = [
    "ì´ ì¢…ëª©ì˜ íˆ¬ì ë§¤ë ¥ë„ëŠ” ì–´ë–¤ê°€ìš”?",
    "PERê³¼ PBR ì§€í‘œë¥¼ ì–´ë–»ê²Œ í•´ì„í•´ì•¼ í•˜ë‚˜ìš”?",
    "í˜„ì¬ ì¬ë¬´ ìƒíƒœì˜ ê°•ì ê³¼ ì•½ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ì´ ì¢…ëª©ì˜ ë¦¬ìŠ¤í¬ ìš”ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ë™ì¢… ì—…ê³„ ëŒ€ë¹„ ê²½ìŸë ¥ì€ ì–´ë–¤ê°€ìš”?"
]

col1, col2 = st.columns([3, 1])

with col1:
    user_question = st.text_input("ğŸ§  ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:", 
                                placeholder="ì˜ˆ: ì´ ì¢…ëª©ì˜ PERì´ ë†’ìœ¼ë©´ ì–´ë–¤ í•´ì„ì´ ê°€ëŠ¥í•´?")

with col2:
    selected_preset = st.selectbox("ğŸ“‹ ë¯¸ë¦¬ ì •ì˜ëœ ì§ˆë¬¸:", ["ì§ì ‘ ì…ë ¥"] + preset_questions)

if selected_preset != "ì§ì ‘ ì…ë ¥":
    user_question = selected_preset

if user_question:
    if st.session_state.get('model') is not None:
        if st.button("ğŸ” AI ë¶„ì„ ìš”ì²­", type="primary"):
            with st.spinner("ğŸ¤– AIê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                ai_response = generate_ai_response(
                    st.session_state.model, 
                    st.session_state.tokenizer, 
                    user_question, 
                    ì¢…ëª©_df
                )
            
            st.markdown("#### ğŸ¯ AI ë¶„ì„ ê²°ê³¼")
            st.markdown(ai_response)
            
            # ë¶„ì„ ê²°ê³¼ ì €ì¥ ì˜µì…˜
            analysis_text = f"""
# {ì„ íƒí•œ_ì¢…ëª©} AI ë¶„ì„ ê²°ê³¼

**ì§ˆë¬¸:** {user_question}

**AI ë¶„ì„:**
{ai_response}

**ê¸°ë³¸ ì •ë³´:**
- ì¢…ëª©ëª…: {ì¢…ëª©_df['ì¢…ëª©ëª…']}
- í‹°ì»¤: {ì¢…ëª©_df['í‹°ì»¤']}
- í˜„ì¬ê°€: {ì¢…ëª©_df['í˜„ì¬ê°€']:,.0f}ì›
- PER: {ì¢…ëª©_df['PER_ìµœê·¼']:.2f}
- PBR: {ì¢…ëª©_df['PBR_ìµœê·¼']:.2f}
- ROE: {ì¢…ëª©_df['ROE_ìµœê·¼']:.2f}%
"""
            
            st.download_button(
                label="ğŸ“¥ ë¶„ì„ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ",
                data=analysis_text,
                file_name=f"{ì„ íƒí•œ_ì¢…ëª©}_AIë¶„ì„_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.md",
                mime="text/markdown"
            )
    else:
        st.write("ğŸ” AI ëª¨ë¸ ë¡œë”© ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
        # í–¥í›„ vectorstore ê²€ìƒ‰ + LLM ìš”ì•½ìœ¼ë¡œ ì—°ê²° ê°€ëŠ¥
