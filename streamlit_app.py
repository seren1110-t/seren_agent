# kospi_research_app.py
import os

# PyTorchì™€ Streamlit í˜¸í™˜ì„± ë¬¸ì œ í•´ê²° (ë°˜ë“œì‹œ ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# PyTorch ì„í¬íŠ¸ ë° í˜¸í™˜ì„± ìˆ˜ì •
import torch
try:
    # ë°©ë²• 1: torch.classes ê²½ë¡œ ìˆ˜ì •
    torch.classes.__path__ = [os.path.join(torch.__path__[0], 'classes')]
except Exception:
    try:
        # ë°©ë²• 2: ëŒ€ì•ˆ ê²½ë¡œ ì„¤ì •
        if hasattr(torch, 'classes'):
            torch.classes.__path__._path = [os.path.join(torch.__path__[0], 'classes')]
    except Exception:
        pass

import streamlit as st
import pandas as pd
import sqlite3
import gdown
import zipfile
import tarfile
import torch.quantization
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import numpy as np

st.set_page_config(page_title="ğŸ“ˆ KOSPI Analyst AI", layout="wide")

# Google Drive íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ë™ì  ì–‘ìí™” ì ìš© í•¨ìˆ˜
@st.cache_resource
def download_and_load_models():
    """Google Driveì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë™ì  ì–‘ìí™” ì ìš©í•˜ì—¬ ë¡œë“œ"""
    
    # Google Drive íŒŒì¼ ID
    base_model_id = "1CGpO7EO64hkUTU_eQQuZXbh-R84inkIc"
    qlora_adapter_id = "1l2F6a5HpmEmdOwTKOpu5UNRQG_jrXeW0"
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        status_text.text("ğŸ”„ ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (1/6)")
        progress_bar.progress(15)
        
        if not os.path.exists("./base_model"):
            base_model_url = f"https://drive.google.com/uc?id={base_model_id}"
            gdown.download(base_model_url, "./my_base_model.tar.gz", quiet=False)
            
            with tarfile.open("./my_base_model.tar.gz", "r:gz") as tar:
                tar.extractall("./base_model/")
        
        # QLoRA ì–´ëŒ‘í„° ë‹¤ìš´ë¡œë“œ
        status_text.text("ğŸ”„ QLoRA ì–´ëŒ‘í„° ë‹¤ìš´ë¡œë“œ ì¤‘... (2/6)")
        progress_bar.progress(30)
        
        if not os.path.exists("./qlora_adapter"):
            qlora_url = f"https://drive.google.com/uc?id={qlora_adapter_id}"
            gdown.download(qlora_url, "./qlora_results.zip", quiet=False)
            
            with zipfile.ZipFile("./qlora_results.zip", 'r') as zip_ref:
                zip_ref.extractall("./qlora_adapter/")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        status_text.text("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘... (3/6)")
        progress_bar.progress(45)
        
        tokenizer = AutoTokenizer.from_pretrained(
            "./base_model",
            trust_remote_code=True,
            use_fast=False  # í˜¸í™˜ì„±ì„ ìœ„í•´ slow tokenizer ì‚¬ìš©
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (ì•ˆì „í•œ ì„¤ì •)
        status_text.text("ğŸ§  ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘... (4/6)")
        progress_bar.progress(60)
        
        base_model = AutoModelForCausalLM.from_pretrained(
            "./base_model",
            torch_dtype=torch.float32,
            device_map="cpu",
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            use_cache=False,  # ìºì‹œ ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
            torch_compile=False  # ì»´íŒŒì¼ ë¹„í™œì„±í™”
        )
        
        # QLoRA ì–´ëŒ‘í„° ì ìš©
        status_text.text("ğŸ”§ QLoRA ì–´ëŒ‘í„° ì ìš© ì¤‘... (5/6)")
        progress_bar.progress(75)
        
        model = PeftModel.from_pretrained(base_model, "./qlora_adapter")
        
        # ë™ì  ì–‘ìí™” ì ìš©
        status_text.text("âš¡ ë™ì  ì–‘ìí™” ì ìš© ì¤‘... (6/6)")
        progress_bar.progress(85)
        
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        model.eval()
        
        # ì•ˆì „í•œ ë™ì  ì–‘ìí™” ì ìš©
        try:
            with torch.no_grad():
                quantized_model = torch.quantization.quantize_dynamic(
                    model,
                    {torch.nn.Linear},
                    dtype=torch.qint8,
                    inplace=False  # ì›ë³¸ ëª¨ë¸ ë³´ì¡´
                )
        except Exception as e:
            st.warning(f"ë™ì  ì–‘ìí™” ì‹¤íŒ¨, ì›ë³¸ ëª¨ë¸ ì‚¬ìš©: {e}")
            quantized_model = model
        
        progress_bar.progress(100)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
        def get_model_size(model):
            """ëª¨ë¸ í¬ê¸° ê³„ì‚° (MB)"""
            try:
                param_size = sum(p.numel() * p.element_size() for p in model.parameters())
                buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
                return (param_size + buffer_size) / 1024 / 1024
            except:
                return 0
        
        original_size = get_model_size(model)
        quantized_size = get_model_size(quantized_model)
        
        if original_size > 0 and quantized_size > 0:
            compression_ratio = original_size / quantized_size
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì›ë³¸ ëª¨ë¸ í¬ê¸°", f"{original_size:.1f} MB")
            with col2:
                st.metric("ì–‘ìí™” ëª¨ë¸ í¬ê¸°", f"{quantized_size:.1f} MB")
            with col3:
                st.metric("ì••ì¶•ë¥ ", f"{compression_ratio:.1f}x")
        
        status_text.text("âœ… ë™ì  ì–‘ìí™” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        st.success("âš¡ ë™ì  ì–‘ìí™”ë¡œ CPU ìµœì í™” ì™„ë£Œ! ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¬ê²Œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤.")
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            for temp_file in ["./my_base_model.tar.gz", "./qlora_results.zip"]:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
        except:
            pass
        
        # UI ì •ë¦¬
        progress_bar.empty()
        status_text.empty()
        
        return quantized_model, tokenizer
        
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        progress_bar.empty()
        status_text.empty()
        return None, None

@st.cache_data
def load_data(db_name="financial_data.db", table_name="financial_data"):
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    try:
        conn = sqlite3.connect(db_name)
        df = pd.read_sql(f"SELECT * FROM {table_name}", conn)
        conn.close()
        
        # ìˆ«ì ì»¬ëŸ¼ë“¤ì„ numericìœ¼ë¡œ ë³€í™˜í•˜ê³  NaN ì²˜ë¦¬
        numeric_columns = ['PER_ìµœê·¼', 'PBR_ìµœê·¼', 'ROE_ìµœê·¼', 'ë¶€ì±„ë¹„ìœ¨_ìµœê·¼', 'í˜„ì¬ê°€', 
                          'ìœ ë³´ìœ¨_ìµœê·¼', 'ë§¤ì¶œì•¡_ìµœê·¼', 'ì˜ì—…ì´ìµ_ìµœê·¼', 'ìˆœì´ìµ_ìµœê·¼']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col] = df[col].fillna(0)
        
        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def generate_ai_response(model, tokenizer, question, company_data):
    """ì•ˆì „í•œ AI ì‘ë‹µ ìƒì„±"""
    if model is None or tokenizer is None:
        return "AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    # íšŒì‚¬ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    company_info = f"""
    ì¢…ëª©ëª…: {company_data['ì¢…ëª©ëª…']}
    í‹°ì»¤: {company_data['í‹°ì»¤']}
    í˜„ì¬ê°€: {company_data['í˜„ì¬ê°€']:,.0f}ì›
    PER: {company_data['PER_ìµœê·¼']:.2f}
    PBR: {company_data['PBR_ìµœê·¼']:.2f}
    ROE: {company_data['ROE_ìµœê·¼']:.2f}%
    ë¶€ì±„ë¹„ìœ¨: {company_data['ë¶€ì±„ë¹„ìœ¨_ìµœê·¼']:.2f}%
    """
    
    prompt = f"""ë‹¤ìŒì€ {company_data['ì¢…ëª©ëª…']}ì˜ ì¬ë¬´ ì •ë³´ì…ë‹ˆë‹¤:

{company_info}

ì§ˆë¬¸: {question}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ì¦ê¶Œ ë¶„ì„ê°€ ê´€ì ì—ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""
    
    try:
        # ì•ˆì „í•œ í† í°í™”
        inputs = tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512,
            padding=True,
            add_special_tokens=True
        )
        
        # ì•ˆì „í•œ ì¶”ë¡ 
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,  # í† í° ìˆ˜ ì¤„ì—¬ì„œ ì•ˆì •ì„± í–¥ìƒ
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                num_beams=1,
                early_stopping=True,
                use_cache=False
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = response[len(prompt):].strip()
        
        return generated_text if generated_text else "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def get_initial(korean_char):
    """í•œê¸€ ì´ˆì„± ì¶”ì¶œ"""
    try:
        ch_code = ord(korean_char) - ord('ê°€')
        if 0 <= ch_code < 11172:
            cho = ch_code // 588
            return ['ã„±','ã„²','ã„´','ã„·','ã„¸','ã„¹','ã…','ã…‚','ã…ƒ','ã……','ã…†','ã…‡','ã…ˆ','ã…‰','ã…Š','ã…‹','ã…Œ','ã…','ã…'][cho]
    except:
        pass
    return ""

# AI ëª¨ë¸ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
if 'model_loaded' not in st.session_state:
    st.info("ğŸ¤– AI ëª¨ë¸ì„ ì²˜ìŒ ë¡œë“œí•©ë‹ˆë‹¤. ë™ì  ì–‘ìí™”ë¥¼ ì ìš©í•˜ì—¬ CPU ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
    model, tokenizer = download_and_load_models()
    st.session_state.model = model
    st.session_state.tokenizer = tokenizer
    st.session_state.model_loaded = True
    
    if model is not None:
        st.success("âœ… ë™ì  ì–‘ìí™” AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì´ì œ CPUì—ì„œ íš¨ìœ¨ì ì¸ ì§€ëŠ¥í˜• ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        st.rerun()

# ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë™ì¼í•˜ê²Œ ìœ ì§€...
