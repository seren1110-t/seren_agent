import streamlit as st
import sys
import os

# torch import ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ í™˜ê²½ ì„¤ì •
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    st.error(f"Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

st.title("ğŸ¤– ì‘ì€ LLM ëª¨ë¸ ë¡œë“œ í™•ì¸")

if not TRANSFORMERS_AVAILABLE:
    st.stop()

@st.cache_resource
def load_simple_model():
    """ê°€ì¥ ì‘ì€ GPT ëª¨ë¸ ë¡œë“œ"""
    try:
        model_name = "sshleifer/tiny-gpt2"  # ë§¤ìš° ì‘ì€ í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        return model, tokenizer, "âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!"
    except Exception as e:
        return None, None, f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"

# ëª¨ë¸ ë¡œë“œ
with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘..."):
    model, tokenizer, status = load_simple_model()

st.write(status)

if model is not None and tokenizer is not None:
    st.success("ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
    st.subheader("í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸")
    
    prompt = st.text_input("í”„ë¡¬í”„íŠ¸ ì…ë ¥:", "Hello")
    
    if st.button("ìƒì„±"):
        if prompt:
            with st.spinner("ìƒì„± ì¤‘..."):
                try:
                    # í† í°í™”
                    inputs = tokenizer.encode(prompt, return_tensors="pt")
                    
                    # ìƒì„±
                    with torch.no_grad():
                        outputs = model.generate(
                            inputs, 
                            max_length=inputs.shape[1] + 20,
                            num_return_sequences=1,
                            temperature=0.7,
                            do_sample=True,
                            pad_token_id=tokenizer.eos_token_id
                        )
                    
                    # ë””ì½”ë”©
                    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    
                    st.write("**ìƒì„± ê²°ê³¼:**")
                    st.write(result)
                    
                except Exception as e:
                    st.error(f"ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        else:
            st.warning("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
else:
    st.error("ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ì‹œìŠ¤í…œ ì •ë³´
st.sidebar.write("**ì‹œìŠ¤í…œ ì •ë³´:**")
st.sidebar.write(f"Python: {sys.version.split()[0]}")

if TRANSFORMERS_AVAILABLE:
    st.sidebar.write(f"PyTorch: {torch.__version__}")
    st.sidebar.write(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
else:
    st.sidebar.write("PyTorch: ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨")
