import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import zipfile
import os
import tempfile
import gdown
import time
from pathlib import Path
import json

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="KoAlpaca CPU ëª¨ë¸ í…ŒìŠ¤íŠ¸",
    page_icon="ğŸ¤–",
    layout="wide"
)

# êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ ID (ê³µìœ  ë§í¬ì—ì„œ ì¶”ì¶œ)
# https://drive.google.com/file/d/YOUR_FILE_ID/view?usp=sharing ì—ì„œ YOUR_FILE_ID ë¶€ë¶„
GDRIVE_FILE_ID = "1RilOqw77G_Kr9_Cl6kR2UTuBa3uvDWQ8"  # ì‹¤ì œ íŒŒì¼ IDë¡œ ë³€ê²½ í•„ìš”

@st.cache_resource
def download_and_extract_model():
    """êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ"""
    try:
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        temp_dir = tempfile.mkdtemp()
        zip_path = os.path.join(temp_dir, "koalpaca_cpu_deployment.zip")
        extract_path = os.path.join(temp_dir, "koalpaca_cpu_deployment")
        
        # êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ë‹¤ìš´ë¡œë“œ
        st.info("ğŸ”„ êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        download_url = f"https://drive.google.com/uc?id={GDRIVE_FILE_ID}"
        gdown.download(download_url, zip_path, quiet=False)
        
        # ì••ì¶• í•´ì œ
        st.info("ğŸ“¦ ëª¨ë¸ ì••ì¶• í•´ì œ ì¤‘...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        
        # ì••ì¶• í•´ì œëœ í´ë” êµ¬ì¡° í™•ì¸
        model_path = None
        for root, dirs, files in os.walk(extract_path):
            if "tokenizer.json" in files or "config.json" in files:
                model_path = root
                break
        
        if model_path is None:
            # ì²« ë²ˆì§¸ í•˜ìœ„ ë””ë ‰í† ë¦¬ë¥¼ ëª¨ë¸ ê²½ë¡œë¡œ ì‚¬ìš©
            subdirs = [d for d in os.listdir(extract_path) if os.path.isdir(os.path.join(extract_path, d))]
            if subdirs:
                model_path = os.path.join(extract_path, subdirs[0])
            else:
                model_path = extract_path
        
        st.success(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_path}")
        return model_path
        
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

@st.cache_resource
def load_koalpaca_model(model_path):
    """KoAlpaca CPU ëª¨ë¸ ë¡œë“œ"""
    try:
        st.info("ğŸ§  KoAlpaca ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            use_fast=False
        )
        
        # ëª¨ë¸ ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32,
            device_map="cpu",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        model.eval()
        
        st.success("âœ… KoAlpaca ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        return model, tokenizer
        
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None, None

def generate_response(model, tokenizer, question, max_new_tokens=200):
    """ì‘ë‹µ ìƒì„±"""
    try:
        # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        prompt = f"### ì§ˆë¬¸: {question}\n\n### ë‹µë³€:"
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            prompt, 
            return_tensors="pt",
            max_length=512,
            truncation=True,
            padding=True
        )
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1,
                no_repeat_ngram_size=3
            )
        
        # ì‘ë‹µ ë””ì½”ë”©
        generated_ids = outputs[0][len(inputs.input_ids[0]):]
        response = tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        return response.strip()
        
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def main():
    st.title("ğŸ¤– KoAlpaca CPU ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    st.markdown("êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ KoAlpaca CPU ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ê²€ìƒ‰ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    
    # ì‚¬ì´ë“œë°”ì— ì •ë³´ í‘œì‹œ
    with st.sidebar:
        st.header("ğŸ“‹ ëª¨ë¸ ì •ë³´")
        st.info("""
        **ëª¨ë¸**: KoAlpaca-Polyglot-5.8B CPU ë²„ì „
        **ìš©ë„**: í•œêµ­ì–´ ì§ˆì˜ì‘ë‹µ
        **ìµœì í™”**: CPU ì¶”ë¡ ìš©
        """)
        
        st.header("ğŸ”§ ì„¤ì •")
        max_tokens = st.slider("ìµœëŒ€ í† í° ìˆ˜", 50, 500, 200)
        
        st.header("ğŸ“ ì‚¬ìš©ë²•")
        st.markdown("""
        1. ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤
        2. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”
        3. ìƒì„± ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
        """)
    
    # êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ ID ì…ë ¥
    st.header("ğŸ”— êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì„¤ì •")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        file_id = st.text_input(
            "êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ ID ì…ë ¥",
            value=GDRIVE_FILE_ID,
            help="êµ¬ê¸€ ë“œë¼ì´ë¸Œ ê³µìœ  ë§í¬ì—ì„œ íŒŒì¼ IDë¥¼ ì¶”ì¶œí•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”"
        )
    
    with col2:
        st.markdown("**íŒŒì¼ ID ì¶”ì¶œ ë°©ë²•:**")
        st.code("https://drive.google.com/file/d/FILE_ID/view")
        st.caption("FILE_ID ë¶€ë¶„ì„ ë³µì‚¬í•˜ì„¸ìš”")
    
    if not file_id or file_id == "YOUR_GOOGLE_DRIVE_FILE_ID_HERE":
        st.warning("âš ï¸ êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()
    
    # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
    global GDRIVE_FILE_ID
    GDRIVE_FILE_ID = file_id
    
    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
    st.header("ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ")
    
    if st.button("ğŸ”„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘", type="primary"):
        # ìºì‹œ í´ë¦¬ì–´
        st.cache_resource.clear()
        
        with st.spinner("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
            model_path = download_and_extract_model()
            
            if model_path:
                model, tokenizer = load_koalpaca_model(model_path)
                
                if model and tokenizer:
                    st.session_state.model = model
                    st.session_state.tokenizer = tokenizer
                    st.session_state.model_loaded = True
                    st.success("ğŸ‰ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì´ì œ ì§ˆë¬¸ì„ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    st.error("âŒ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.error("âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    # ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if not hasattr(st.session_state, 'model_loaded') or not st.session_state.model_loaded:
        st.info("ğŸ‘† ë¨¼ì € 'ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    # ì§ˆì˜ì‘ë‹µ ì¸í„°í˜ì´ìŠ¤
    st.header("ğŸ’¬ ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸")
    
    # ì˜ˆì‹œ ì§ˆë¬¸ë“¤
    example_questions = [
        "ì‚¼ì„±ì „ìì˜ ì¬ë¬´ìƒíƒœëŠ” ì–´ë–»ìŠµë‹ˆê¹Œ?",
        "ë„¤ì´ë²„ì˜ ì‚¬ì—… ì „ë§ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.",
        "ì¹´ì¹´ì˜¤ì˜ ì£¼ìš” ì‚¬ì—… ì˜ì—­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í•œêµ­ IT ì‚°ì—…ì˜ í˜„í™©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "íˆ¬ìí•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ìš”ì†Œë“¤ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        question = st.text_area(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
            height=100,
            placeholder="ì˜ˆ: ì‚¼ì„±ì „ìì˜ ì¬ë¬´ìƒíƒœëŠ” ì–´ë–»ìŠµë‹ˆê¹Œ?"
        )
    
    with col2:
        st.markdown("**ì˜ˆì‹œ ì§ˆë¬¸:**")
        for i, ex_q in enumerate(example_questions):
            if st.button(f"ì˜ˆì‹œ {i+1}", key=f"example_{i}"):
                question = ex_q
                st.rerun()
    
    # ì‘ë‹µ ìƒì„±
    col1, col2, col3 = st.columns([1, 1, 2])
    
    with col1:
        generate_btn = st.button("ğŸš€ ì‘ë‹µ ìƒì„±", type="primary")
    
    with col2:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
            if 'chat_history' in st.session_state:
                del st.session_state.chat_history
            st.rerun()
    
    if generate_btn and question.strip():
        if hasattr(st.session_state, 'model') and st.session_state.model:
            with st.spinner("ğŸ¤” ì‘ë‹µ ìƒì„± ì¤‘..."):
                start_time = time.time()
                
                response = generate_response(
                    st.session_state.model, 
                    st.session_state.tokenizer, 
                    question,
                    max_tokens
                )
                
                end_time = time.time()
                generation_time = end_time - start_time
                
                # ëŒ€í™” ê¸°ë¡ ì €ì¥
                if 'chat_history' not in st.session_state:
                    st.session_state.chat_history = []
                
                st.session_state.chat_history.append({
                    'question': question,
                    'response': response,
                    'time': generation_time
                })
        else:
            st.error("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    if hasattr(st.session_state, 'chat_history') and st.session_state.chat_history:
        st.header("ğŸ“œ ëŒ€í™” ê¸°ë¡")
        
        for i, chat in enumerate(reversed(st.session_state.chat_history)):
            with st.expander(f"ğŸ’¬ ëŒ€í™” {len(st.session_state.chat_history) - i}", expanded=(i == 0)):
                st.markdown(f"**â“ ì§ˆë¬¸:** {chat['question']}")
                st.markdown(f"**ğŸ¤– ì‘ë‹µ:** {chat['response']}")
                st.caption(f"â±ï¸ ìƒì„± ì‹œê°„: {chat['time']:.2f}ì´ˆ")
    
    # ì‹œìŠ¤í…œ ì •ë³´
    with st.expander("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´"):
        st.json({
            "Python ë²„ì „": "3.8+",
            "PyTorch ë²„ì „": torch.__version__,
            "ë””ë°”ì´ìŠ¤": "CPU",
            "ëª¨ë¸ ìƒíƒœ": "ë¡œë“œë¨" if hasattr(st.session_state, 'model_loaded') and st.session_state.model_loaded else "ë¡œë“œ ì•ˆë¨"
        })

if __name__ == "__main__":
    main()
