# -*- coding: utf-8 -*-
"""
ë°ì´í„° ìˆ˜ì§‘ ì—ì´ì „íŠ¸ - ê¸°ì¡´ ë‰´ìŠ¤/ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ê´€ë¦¬
"""

import asyncio
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum
import torch
from langgraph.graph import StateGraph, END
from langchain_core.language_models import BaseLLM
from langchain_openai import ChatOpenAI

# ê¸°ì¡´ ëª¨ë“ˆë“¤ ì„í¬íŠ¸ (ì›ë³¸ ì½”ë“œë¥¼ ëª¨ë“ˆí™”)
from news_collector import (
    scrape_news_until_cutoff_today, 
    clean_korean_text,
    HuggingFaceEmbeddings,
    FAISS,
    Document,
    RecursiveCharacterTextSplitter
)
from financial_collector import (
    get_kospi_tickers,
    collect_kospi_reports_async,
    collect_and_save_all
)

class CollectionStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"

@dataclass
class CollectionResult:
    status: CollectionStatus
    message: str
    data_count: int = 0
    file_path: Optional[str] = None
    error: Optional[str] = None

class DataCollectionState:
    """ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ ê´€ë¦¬"""
    def __init__(self):
        self.news_result: Optional[CollectionResult] = None
        self.financial_result: Optional[CollectionResult] = None
        self.vector_result: Optional[CollectionResult] = None
        self.start_time: datetime = datetime.now()
        self.context: Dict[str, Any] = {}
        self.error_log: List[str] = []

class SmartDataCollectorAgent:
    """ì§€ëŠ¥í˜• ë°ì´í„° ìˆ˜ì§‘ ì—ì´ì „íŠ¸"""
    
    def __init__(self, llm: BaseLLM = None):
        self.llm = llm or ChatOpenAI(model="gpt-4", temperature=0)
        self.state = DataCollectionState()
        
    def analyze_collection_context(self) -> Dict[str, Any]:
        """í˜„ì¬ ìˆ˜ì§‘ ìƒí™©ì„ ë¶„ì„í•˜ì—¬ ìµœì  ì „ëµ ê²°ì •"""
        
        context_prompt = f"""
        í˜„ì¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        
        ë‹¤ìŒ ì¡°ê±´ë“¤ì„ ë¶„ì„í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘ ì „ëµì„ ê²°ì •í•´ì£¼ì„¸ìš”:
        
        1. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€:
           - financial_data.db: {os.path.exists('financial_data.db')}
           - bk_faiss_index/: {os.path.exists('bk_faiss_index')}
           - bk_docs.pkl: {os.path.exists('bk_docs.pkl')}
           
        2. ì‹œì¥ ìƒí™© (í˜„ì¬ ì‹œê°„ ê¸°ì¤€):
           - ì£¼ì‹ ì‹œì¥ ê°œì¥ ì‹œê°„: 09:00-15:30
           - ë‰´ìŠ¤ ìˆ˜ì§‘ ê¸°ì¤€ ì‹œê°„: 09:00 ì´í›„
           
        3. ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„ (íŒŒì¼ ìˆ˜ì • ì‹œê°„ ê¸°ì¤€):
           - DB íŒŒì¼: {self._get_file_age('financial_data.db')}
           - ë²¡í„° DB: {self._get_file_age('bk_faiss_index')}
        
        ì‘ë‹µ í˜•ì‹:
        {{
            "news_collection": "í•„ìš”í•¨/ë¶ˆí•„ìš”í•¨/ì¡°ê±´ë¶€",
            "financial_collection": "í•„ìš”í•¨/ë¶ˆí•„ìš”í•¨/ì¡°ê±´ë¶€", 
            "priority": "news/financial/both",
            "reason": "íŒë‹¨ ê·¼ê±°",
            "recommended_schedule": "ì¦‰ì‹œ/1ì‹œê°„í›„/ë‚´ì¼"
        }}
        """
        
        try:
            response = self.llm.invoke(context_prompt)
            # ì‹¤ì œë¡œëŠ” ì‘ë‹µì„ íŒŒì‹±í•´ì•¼ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ ë¡œì§ ì‚¬ìš©
            return self._default_collection_strategy()
        except Exception as e:
            print(f"LLM ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ ì „ëµ ì‚¬ìš©: {e}")
            return self._default_collection_strategy()
    
    def _default_collection_strategy(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ìˆ˜ì§‘ ì „ëµ"""
        current_hour = datetime.now().hour
        
        # ê¸°ë³¸ ì „ëµ: ì˜¤ì „ 9ì‹œ ì´í›„ì—ëŠ” ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘
        if current_hour >= 9:
            return {
                "news_collection": "í•„ìš”í•¨",
                "financial_collection": "í•„ìš”í•¨",
                "priority": "both",
                "reason": "ì •ê·œ ìˆ˜ì§‘ ì‹œê°„",
                "recommended_schedule": "ì¦‰ì‹œ"
            }
        else:
            return {
                "news_collection": "ë¶ˆí•„ìš”í•¨",
                "financial_collection": "ë¶ˆí•„ìš”í•¨", 
                "priority": "none",
                "reason": "ì‹œì¥ ê°œì¥ ì „",
                "recommended_schedule": "9ì‹œ ì´í›„"
            }
    
    def _get_file_age(self, filepath: str) -> str:
        """íŒŒì¼ì˜ ë‚˜ì´ë¥¼ ë°˜í™˜"""
        if not os.path.exists(filepath):
            return "íŒŒì¼ ì—†ìŒ"
        
        mtime = os.path.getmtime(filepath)
        file_time = datetime.fromtimestamp(mtime)
        age = datetime.now() - file_time
        
        if age.days > 0:
            return f"{age.days}ì¼ ì „"
        elif age.seconds > 3600:
            return f"{age.seconds // 3600}ì‹œê°„ ì „"
        else:
            return f"{age.seconds // 60}ë¶„ ì „"

class NewsCollectionAgent:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ì „ìš© ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
    def execute(self, state: DataCollectionState) -> CollectionResult:
        """ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
        
        try:
            print("ğŸ“° ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
            
            # 1. ë‰´ìŠ¤ í¬ë¡¤ë§ (ê¸°ì¡´ ì½”ë“œ ì‚¬ìš©)
            news_data = scrape_news_until_cutoff_today(cutoff_hour=9)
            
            if not news_data:
                return CollectionResult(
                    status=CollectionStatus.SKIPPED,
                    message="ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    data_count=0
                )
            
            # 2. ë°ì´í„° ì •ì œ
            df = pd.DataFrame(news_data)
            df['ì œëª©'] = df['ì œëª©'].apply(clean_korean_text)
            df['ë³¸ë¬¸'] = df['ë³¸ë¬¸'].apply(clean_korean_text)
            df.dropna(subset=['ì œëª©', 'ë³¸ë¬¸'], inplace=True)
            df['ë‚´ìš©'] = df['ì œëª©'] + '\n' + df['ë³¸ë¬¸']
            df['ë‚´ìš©'] = df['ë‚´ìš©'].str.slice(0, 500)
            
            # 3. ë²¡í„° DB ì—…ë°ì´íŠ¸
            self._update_vector_db(df)
            
            return CollectionResult(
                status=CollectionStatus.COMPLETED,
                message="ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë²¡í„° DB ì—…ë°ì´íŠ¸ ì™„ë£Œ",
                data_count=len(df),
                file_path="bk_faiss_index"
            )
            
        except Exception as e:
            return CollectionResult(
                status=CollectionStatus.FAILED,
                message="ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨",
                error=str(e)
            )
    
    def _update_vector_db(self, df):
        """ë²¡í„° DB ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ë¡œì§)"""
        
        embedding_model = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={'device': self.device}
        )
        
        # ê¸°ì¡´ ë¬¸ì„œ ë¡œë“œ
        try:
            with open("bk_docs.pkl", "rb") as f:
                bk_docs = pickle.load(f)
            bk_faiss_db = FAISS.load_local("bk_faiss_index", embedding_model, 
                                         allow_dangerous_deserialization=True)
        except:
            bk_docs = []
            bk_faiss_db = None
        
        # ì¤‘ë³µ ì œê±° ë° ìƒˆ ë¬¸ì„œ ì¶”ê°€
        existing_dates = set(doc.metadata.get("ì¼ì") for doc in bk_docs if "ì¼ì" in doc.metadata)
        
        new_docs = []
        for idx, row in df.iterrows():
            news_date = row["ë‚ ì§œ"]
            if news_date in existing_dates:
                continue
            metadata = {"ì¼ì": news_date, "ì œëª©": row["ì œëª©"], "URL": row["URL"]}
            new_docs.append(Document(page_content=row["ë‚´ìš©"], metadata=metadata))
        
        if new_docs:
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=100)
            new_split_docs = text_splitter.split_documents(new_docs)
            bk_docs.extend(new_split_docs)
            
            if bk_faiss_db is None:
                bk_faiss_db = FAISS.from_documents(new_split_docs, embedding_model)
            else:
                bk_faiss_db.add_documents(new_split_docs)
            
            # ì €ì¥
            with open("bk_docs.pkl", "wb") as f:
                pickle.dump(bk_docs, f)
            bk_faiss_db.save_local("bk_faiss_index")

class FinancialCollectionAgent:
    """ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ ì „ìš© ì—ì´ì „íŠ¸"""
    
    def execute(self, state: DataCollectionState) -> CollectionResult:
        """ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
        
        try:
            print("ğŸ’° ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
            
            # ê¸°ì¡´ collect_and_save_all() í•¨ìˆ˜ ì‚¬ìš©
            collect_and_save_all()
            
            # DB íŒŒì¼ í™•ì¸
            if os.path.exists("financial_data.db"):
                import sqlite3
                conn = sqlite3.connect("financial_data.db")
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM financial_data")
                count = cursor.fetchone()[0]
                conn.close()
                
                return CollectionResult(
                    status=CollectionStatus.COMPLETED,
                    message="ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ",
                    data_count=count,
                    file_path="financial_data.db"
                )
            else:
                return CollectionResult(
                    status=CollectionStatus.FAILED,
                    message="DB íŒŒì¼ ìƒì„± ì‹¤íŒ¨"
                )
                
        except Exception as e:
            return CollectionResult(
                status=CollectionStatus.FAILED,
                message="ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨",
                error=str(e)
            )

# ============ LangGraph ì›Œí¬í”Œë¡œìš° ì •ì˜ ============

def create_data_collection_workflow():
    """ë°ì´í„° ìˆ˜ì§‘ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    
    # ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    smart_agent = SmartDataCollectorAgent()
    news_agent = NewsCollectionAgent()
    financial_agent = FinancialCollectionAgent()
    
    def analyze_context_node(state: dict):
        """ìƒí™© ë¶„ì„ ë…¸ë“œ"""
        print("ğŸ” ë°ì´í„° ìˆ˜ì§‘ ìƒí™© ë¶„ì„ ì¤‘...")
        
        context = smart_agent.analyze_collection_context()
        state["collection_strategy"] = context
        state["analysis_complete"] = True
        
        print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼: {context['reason']}")
        return state
    
    def collect_news_node(state: dict):
        """ë‰´ìŠ¤ ìˆ˜ì§‘ ë…¸ë“œ"""
        strategy = state.get("collection_strategy", {})
        
        if strategy.get("news_collection") == "í•„ìš”í•¨":
            result = news_agent.execute(smart_agent.state)
            state["news_result"] = result
            print(f"ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ê²°ê³¼: {result.message}")
        else:
            state["news_result"] = CollectionResult(
                status=CollectionStatus.SKIPPED,
                message="ë‰´ìŠ¤ ìˆ˜ì§‘ ë¶ˆí•„ìš”"
            )
            print("ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤í‚µ")
        
        return state
    
    def collect_financial_node(state: dict):
        """ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ ë…¸ë“œ"""
        strategy = state.get("collection_strategy", {})
        
        if strategy.get("financial_collection") == "í•„ìš”í•¨":
            result = financial_agent.execute(smart_agent.state)
            state["financial_result"] = result
            print(f"ğŸ’° ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼: {result.message}")
        else:
            state["financial_result"] = CollectionResult(
                status=CollectionStatus.SKIPPED,
                message="ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ ë¶ˆí•„ìš”"
            )
            print("ğŸ’° ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í‚µ")
        
        return state
    
    def finalize_node(state: dict):
        """ìµœì¢… ì •ë¦¬ ë…¸ë“œ"""
        news_result = state.get("news_result")
        financial_result = state.get("financial_result")
        
        # ê²°ê³¼ ìš”ì•½
        summary = {
            "execution_time": datetime.now(),
            "news_status": news_result.status.value if news_result else "unknown",
            "financial_status": financial_result.status.value if financial_result else "unknown",
            "total_news_count": news_result.data_count if news_result else 0,
            "total_financial_count": financial_result.data_count if financial_result else 0,
            "generated_files": []
        }
        
        # ìƒì„±ëœ íŒŒì¼ í™•ì¸
        expected_files = ["financial_data.db", "bk_faiss_index", "bk_docs.pkl"]
        for file in expected_files:
            if os.path.exists(file):
                summary["generated_files"].append(file)
        
        state["final_summary"] = summary
        
        print("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {summary}")
        
        return state
    
    # ì›Œí¬í”Œë¡œìš° êµ¬ì„±
    workflow = StateGraph(dict)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("analyze_context", analyze_context_node)
    workflow.add_node("collect_news", collect_news_node)
    workflow.add_node("collect_financial", collect_financial_node)
    workflow.add_node("finalize", finalize_node)
    
    # ì—£ì§€ ì—°ê²°
    workflow.set_entry_point("analyze_context")
    workflow.add_edge("analyze_context", "collect_news")
    workflow.add_edge("collect_news", "collect_financial")
    workflow.add_edge("collect_financial", "finalize")
    workflow.add_edge("finalize", END)
    
    return workflow.compile()

# ============ ì‹¤í–‰ í•¨ìˆ˜ ============

def run_data_collection_agent():
    """ë°ì´í„° ìˆ˜ì§‘ ì—ì´ì „íŠ¸ ì‹¤í–‰"""
    
    print("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì—ì´ì „íŠ¸ ì‹œì‘!")
    print(f"â° ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    app = create_data_collection_workflow()
    
    initial_state = {
        "start_time": datetime.now(),
        "execution_mode": "auto"
    }
    
    try:
        final_state = app.invoke(initial_state)
        
        # ì‹¤í–‰ ê²°ê³¼ ë¦¬í¬íŠ¸
        summary = final_state.get("final_summary", {})
        
        print("\n" + "="*50)
        print("ğŸ“‹ ì‹¤í–‰ ê²°ê³¼ ë¦¬í¬íŠ¸")
        print("="*50)
        print(f"ë‰´ìŠ¤ ìˆ˜ì§‘: {summary.get('news_status', 'Unknown')}")
        print(f"ì¬ë¬´ ìˆ˜ì§‘: {summary.get('financial_status', 'Unknown')}")
        print(f"ë‰´ìŠ¤ ê°œìˆ˜: {summary.get('total_news_count', 0)}")
        print(f"ì¬ë¬´ ë°ì´í„° ê°œìˆ˜: {summary.get('total_financial_count', 0)}")
        print(f"ìƒì„±ëœ íŒŒì¼: {', '.join(summary.get('generated_files', []))}")
        print("="*50)
        
        return final_state
        
    except Exception as e:
        print(f"âŒ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return None

if __name__ == "__main__":
    # GitHub Actionsë‚˜ cronì—ì„œ ì‹¤í–‰ë  ë•Œ
    run_data_collection_agent()
